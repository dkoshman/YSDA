{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep CNN for computer vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dimakoshman/.pyenv/versions/shad_env/lib/python3.9/site-packages/pandas/compat/__init__.py:124: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named '_lzma'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5w/gytwm8sd6vldh9ym3dvj_x_m0000gn/T/ipykernel_13617/3036322190.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/shad_env/lib/python3.9/site-packages/torchvision/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/shad_env/lib/python3.9/site-packages/torchvision/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlsun\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSUN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLSUNClass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfolder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImageFolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetFolder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcoco\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCocoCaptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCocoDetection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcifar\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCIFAR10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCIFAR100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mstl10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSTL10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/shad_env/lib/python3.9/site-packages/torchvision/datasets/lsun.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mverify_str_arg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable_to_str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/shad_env/lib/python3.9/site-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murlparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mlzma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.7/lib/python3.9/lzma.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0m_lzma\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_lzma\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_encode_filter_properties\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_decode_filter_properties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_compression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named '_lzma'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import show_images\n",
    "\n",
    "\n",
    "# autoreload ALL modules in real time\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named '_lzma'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5w/gytwm8sd6vldh9ym3dvj_x_m0000gn/T/ipykernel_13617/1607350704.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mlzma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.9.7/lib/python3.9/lzma.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0m_lzma\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_lzma\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_encode_filter_properties\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_decode_filter_properties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_compression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named '_lzma'"
     ]
    }
   ],
   "source": [
    "import lzma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your imports\n",
    "\n",
    "num_workers = 2     # maximum number of subprocces (check https://pytorch.org/docs/stable/data.html for more info)\n",
    "batch_size = 16     # increase or decrease batch_size here\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task #1: Basic Pipeline [3 points]\n",
    "\n",
    "## Step 1: Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) from torchvision.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# download data\n",
    "train_data = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, \n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_data = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, \n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataloaders and plot some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create dataloaders for model\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_data, shuffle=True, \n",
    "    batch_size=batch_size, num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_data, shuffle=True, \n",
    "    batch_size=batch_size, num_workers=num_workers\n",
    ")\n",
    "\n",
    "# show some images from CIFAR\n",
    "labels_map = {\n",
    "    0: \"Airplane\",\n",
    "    1: \"Automobile\",\n",
    "    2: \"Bird\",\n",
    "    3: \"Cat\",\n",
    "    4: \"Deer\",\n",
    "    5: \"Dog\",\n",
    "    6: \"Frog\",\n",
    "    7: \"Horse\",\n",
    "    8: \"Ship\",\n",
    "    9: \"Truck\",\n",
    "}\n",
    "\n",
    "N_samples = 16\n",
    "images, labels = next(iter(train_dataloader))\n",
    "show_images(\n",
    "    images[:N_samples], \n",
    "    [labels_map[i.item()] for i in labels[:N_samples]], \n",
    "    transform=transforms.ToPILImage()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint!**\n",
    "\n",
    "Use can use ```torchvision.utils.make_grid()``` for your own plots. Check [documentation](https://pytorch.org/vision/stable/utils.html) for examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Neural Network [1 point]\n",
    "\n",
    "\n",
    "В этом задании вам предстоит заполнить пробелы в типичном pipeline для обучения нейросетей на pytorch. \n",
    "\n",
    "Для наглядности (и последующего сравнения с Deep CNN-сетями) будем использовать простейший перцептрон в качестве модели. \n",
    "\n",
    "**Hint!** Можно изменить размеры и число скрытых слоёв, передав в качестве аргумента ```blocks``` их список."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN Architecrute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import MLP\n",
    "\n",
    "net = MLP(images[0].shape, n_classes=len(labels_map), blocks=[256, 512, 256, 128, 64]).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint!** Можно обратиться к документации [pytorch](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html) для примеров реализации функций ```train_loop()``` и ```test_loop()```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1 point ] Train-test loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "from math import ceil\n",
    "\n",
    "\n",
    "def train_loop(model, dataloader, loss_fn, optimizer, step=0.05):\n",
    "    out = display(IPython.display.Pretty('Learning...'), display_id=True)\n",
    "\n",
    "    size = len(dataloader.dataset) \n",
    "    len_size = len(str(size))\n",
    "    batches = ceil(size / dataloader.batch_size) - 1\n",
    "    \n",
    "    percentage = 0\n",
    "    for batch, (X, y) in enumerate(tqdm(dataloader, leave=False, desc=\"Batch #\")):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # evaluate\n",
    "        # <----- your code here ----->\n",
    "        \n",
    "        # backpropagation\n",
    "        # <----- your code here ----->\n",
    "        \n",
    "        # print info\n",
    "        if batch / batches > percentage or batch == batches: \n",
    "            out.update(f'[{int(percentage * size)}/{size}] Loss: {loss:>8f}')\n",
    "            percentage += step\n",
    "        \n",
    "        \n",
    "def test_loop(model, dataloader, loss_fn):\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss, correct = 0, 0\n",
    "    batches = ceil(size / dataloader.batch_size)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # evalute and check predictions\n",
    "        # <----- your code here ----->\n",
    "\n",
    "    test_loss /= batches\n",
    "    correct /= size\n",
    "    \n",
    "    print(f\"Validation accuracy: {(100*correct):>0.1f}%, Validation loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1 point] Learning curves\n",
    "\n",
    "1. Модифицируйте функции ```train_loop()``` и ```test_loop()``` таким образом, чтобы они возвращали словарь ```history```, содержащий ключи ```val_acc```, ```val_loss```, ```train_acc``` и ```train_loss```. \n",
    "\n",
    "2. Постройте графики зависимости ```loss_fn``` и ```accuracy``` для обучающей и тестовой выборок от эпохи обучения.\n",
    "\n",
    "**Hint!** Не стоит пропускать этот пункт: другие задания могут требовать наличия соответствующих графиков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <----- your code here ----->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train Network\n",
    "\n",
    "**Hint!** В качестве лосс-функции следует использовать ```CrossEntropy```, а в качестве оптимизитора - ```Adam```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn, optimizer and number of epochs are required\n",
    "# <----- your code here ----->\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    # <----- your code here ----->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task #2: ResNet [4 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [3 points: 1 for each correct class]\n",
    "\n",
    "В этом задании от вас потребуется заполнить пропуски в ```./models/ResNet.py``` таким образом, чтобы полученная архитектура соответствовала ```resnet18```. Мы будем использовать именно эту модификацию архитектуры из-за её небольшого размера и относительной простоты самостоятельной реализации.\n",
    "\n",
    "**Hint 0!**\n",
    "В качестве примера можно опираться на соответствующую реализацию ```resnet18``` из **pytorch**.\n",
    "\n",
    "**Hint 1!**\n",
    "Благодаря **autoreload** вы можете использовать свежие изменения в ResNet.py без перезагрузки модуля. \n",
    "\n",
    "**Hint 2!**\n",
    "Первым делом попробуйте сопоставить описанные классы с описанием встроенной в pytorch модели. Не забывайте про последовательную отладку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchvision.models.resnet18() # uncomment this to see a Hint 0!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import ResidualBlock, ResNetLayer, ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test ResidualBlock shapes\n",
    "assert ResidualBlock(64, 64)(torch.rand(1, 64, 32, 32)).shape == torch.Size([1, 64, 32, 32])\n",
    "assert ResidualBlock(64, 128)(torch.rand(1, 64, 32, 32)).shape == torch.Size([1, 128, 16, 16])\n",
    "assert ResidualBlock(128, 256)(torch.rand(100, 128, 16, 16)).shape == torch.Size([100, 256, 8, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test ResNetLayer shapes\n",
    "assert ResNetLayer(64, 64)(torch.rand(1, 64, 32, 32)).shape == torch.Size([1, 64, 32, 32])\n",
    "assert ResNetLayer(64, 128)(torch.rand(1, 64, 32, 32)).shape == torch.Size([1, 128, 16, 16])\n",
    "assert ResNetLayer(128, 256)(torch.rand(100, 128, 16, 16)).shape == torch.Size([100, 256, 8, 8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint 3!**\n",
    "Обратите внимание на структуру ```resnet18```. Первая часть (до появления ```ResNetLayer```) - это блок даунсэмплинга. Не забудьте модифицировать структуру сети так, чтобы она была применима к изображениям из **CIFAR10**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = ResNet18()(images) # эта строчка не должна вызывать ошибку"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите свою модель в течении небольшого количества эпох (6-30). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn, optimizer and number of epochs are required\n",
    "# <----- your code here ----->\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    # <----- your code here ----->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1 point]\n",
    "\n",
    "Сравните процесс обучения свёрточной сети и перцептрона, а также число параметров. Какие выводы можно сделать?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<----- your answer here ----->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task #3: EfficientNet [3 points]\n",
    "\n",
    "Иногда нет необходимости обучать модели \"с нуля\". Попробуем использовать для этой задачи технику, называющуюся **transfer learning**. В отличие от **fine tuning**, мы не будем переобучать всю сеть целиком. Вместо этого мы будем использовать уже предобученную сеть в качестве **fixed feature extractor**. \n",
    "\n",
    "Активации последнего свёрточного слоя мы будем использовать как уже готовые признаки. И уже на этих признаках мы обучим собственный классификатор, подходящий к нашей задаче.\n",
    "\n",
    "Будем использовать обученную на датасете **ImageNet** сеть ```EfficientNet```. \n",
    "\n",
    "Чтобы адаптировать её к нашему датасету, потребуется **\"заморозить\" веса** и заменить классификатор модели. \n",
    "\n",
    "**Hint!** Обратитесь к документации [pytorch](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html#convnet-as-fixed-feature-extractor) за подробностями. Обратите внимание, что мы **не переобучаем всю сеть целиком**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1 point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import efficientnet_b0\n",
    "\n",
    "\n",
    "# load and freeze pretrained model\n",
    "# <----- your code here ----->\n",
    "\n",
    "# change classifier\n",
    "# <----- your code here ----->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn, optimizer and number of epochs are required\n",
    "# <----- your code here ----->\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    # <----- your code here ----->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2 points] Comparsion and tuning\n",
    "\n",
    "Переобучите несколько (не меньше трёх различных) сетей из библиотеки ```torchvision.models```, используя **transfer learning**. Рекомендуется выбрать несколько вариаций одной и той же сети (например, ```resnet``` или ```efficientnet```). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <----- your code here ----->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1 point]\n",
    "Нарисуйте графики зависимости ошибки и точности во время обучения от числа прошедших эпох для всех сетей. \n",
    "\n",
    "**Hint 1!** На одном графике должны быть представлены все сети (а также легенда), но только один из четырёх параметров.\n",
    "\n",
    "**Hint 1.5!** Из предыдущего пункта следует, что графиков должно быть... четыре.\n",
    "\n",
    "**Hint 2!** Воспользуйтесь возможностью создавать ```subplot``` в библиотеке ```matplotlib```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <----- your code here ----->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### [1 point]\n",
    "Сравните результаты. Как размер и глубина сети влияют на обучение?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<----- your answer here ----->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Task #4: Deep Double Descent [5 points]\n",
    "\n",
    "*Несмотря на то, что это задание является бонусным, крайне рекомендуется попробовать его осилить. Оно не настолько страшное, каким кажется со стороны, однако поможет закрепить полученные навыки.*\n",
    "\n",
    "В этом задании вам предлагается познакомиться с эффектом, называемым [deep double descent](https://arxiv.org/abs/1912.02292). Ознакомительную краткую версию можно найти [тут](https://openai.com/blog/deep-double-descent/).\n",
    "\n",
    "\n",
    "## 4.1 Network width impact [3 points]\n",
    "\n",
    "Вы будете исследовать способность нейросети обобщать данные в зависимости от её архитектуры. Чтобы более ясно увидеть эту зависимость, потребуется добавить к обучающим данным некоторое количество шума. \n",
    "\n",
    "<img src=\"./resources/double_descent.png\" width=\"700\"/>\n",
    "\n",
    "### 4.1.1 Adding noise\n",
    "\n",
    "Необходимо подготовить три различных набора данных из уже доступного вам **CIFAR10**:\n",
    "\n",
    "    1. В первом все метки классов правильные.\n",
    "    2. Во втором 10% меток классов случайные.\n",
    "    3. В третьем 20% меток классов случайные. \n",
    "\n",
    "### 4.1.2 Choose basic model\n",
    "\n",
    "В качесте базовой модели будем использовать **ResNet18**. \n",
    "\n",
    "Вам потребуется изменять только количество каналов в каждом свёрточном слое (и, соответственно, размеры линейного слоя в классификаторе). На графике выше по оси абсцисс отмечена \"ширина\" сети, то есть число каналов входящих в сеть свёрток. Глубина сети, то есть число блоков или входных слоёв, должна остаться _неизменной_.\n",
    "\n",
    "Начните с небольших свёрток по 4 нейрона на первом слое **ResNet18** и постепенно увеличивайте количество до 128-256 на первом слое. Число нейронов на последующих слоях должно меняться  пропорционально первому слою. Используйте разумный шаг при увеличении числа нейронов для построения графика.\n",
    "\n",
    "Ваша задача - точно отследить, при каком количестве параметров сеть:\n",
    "\n",
    "    a) Число параметров слишком мало, сеть не обладает обобщающей способностью.\n",
    "    b) Число параметров оптимально или близко к оптимальному (первый локальный минимум на графике). \n",
    "    c) Число параметров больше, чем необходимо (с увеличением числа параметров значения loss-функции после обучения должны стремиться ко второму минимуму).\n",
    "\n",
    "### 4.1.3 Compare models\n",
    "\n",
    "Цель задания - самостоятельно получить такой же график, как и у авторов статьи. Однако в нашем случае мы хотим отрисовать три различных линии на графике: каждой линии должна соответстовать сеть, обученная на своей версии датасета из пункта **4.1.2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <----- your code here ----->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Network samples impact [2 points]\n",
    "\n",
    "Это задание похоже на предыдущее: однако, в этом случае вы будем исследовать зависимость эффекта не от ширины сети, а от размера обучающей выборки. \n",
    "\n",
    "<img src=\"./resources/sample_wise_double_descent.svg\" width=\"700\"/>\n",
    "\n",
    "### 4.2.1 Truncate dataset\n",
    "\n",
    "Необходимо подготовить два различных набора данных из уже доступного вам **CIFAR10**:\n",
    "\n",
    "    1. Полный датасет без изменений.\n",
    "    2. 40% от датасета. \n",
    "    \n",
    "### 4.2.2 Compare models\n",
    "\n",
    "\n",
    "Аналогично **4.1**, глубина и топология сети должны оставаться неизменными; от вас снова требуется изменять только число нейронов в свёртках. Шаг и сетку параметров можно использовать такую же, как и пункте **4.1**.\n",
    "\n",
    "Для каждого значения \"ширины\" сети требуется отметить две точки: значение **loss**-функции при обучении на полном датасете и на частичном. \n",
    "\n",
    "\n",
    "Так же как и в прошлом задании, вы можете опираться на результат авторов статьи (пример расположен выше). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <----- your code here ----->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (DimaKoshman)",
   "language": "python",
   "name": "pycharm-39683df4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
