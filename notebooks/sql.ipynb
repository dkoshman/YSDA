{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0500d009-120f-4ff1-a559-60aeb1109a89",
   "metadata": {},
   "source": [
    "Content\n",
    "- [Database inspection](#Database_inspection)\n",
    "- [Vacuum](#Vacuum)\n",
    "- [Polars SQL read and write](#Polars_SQL_read_and_write)\n",
    "- [Metadata](#Metadata)\n",
    "- [ORM](#ORM)\n",
    "- [Read table in parallel batches](#Read_table_in_parallel_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c4b7ac-dce7-4812-af2c-366ad1e4dac6",
   "metadata": {},
   "source": [
    "Sqlalchemy is useful for database inspection, such as getting primary keys, indices, data types. It is also handy for building database-agnostic queries. It doesn't provide support for all statements though, such as ALTER, which are easier to build manually with the use of f-strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "437ec2b4-1f1e-49b3-8a96-22b851547382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from typing import Literal\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import oracledb\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import psycopg2\n",
    "import sqlalchemy.orm\n",
    "import sqlalchemy as sa\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "14d7fa8b-8807-45b6-a8c6-9df9bc58d0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oracle_driver_name():\n",
    "    return \"oracle+oracledb\"\n",
    "\n",
    "\n",
    "def get_postgres_driver_name():\n",
    "    return \"postgresql+psycopg2\"\n",
    "\n",
    "\n",
    "def to_autocommit_engine(engine):\n",
    "    autocommit_engine = engine.execution_options(isolation_level=\"AUTOCOMMIT\")\n",
    "    return autocommit_engine\n",
    "\n",
    "\n",
    "def execute_query(engine: sa.Engine, query):\n",
    "    query = sa.text(query) if isinstance(query, str) else query\n",
    "    with engine.connect() as connection:\n",
    "        result = connection.execute(query)\n",
    "\n",
    "    return result\n",
    "\n",
    "def get_result_item(result):\n",
    "    result_item = result.scalars().first()\n",
    "    return result_item\n",
    "\n",
    "def compile_statement(\n",
    "    statement, dialect: Literal[\"postgres\", \"oracle\"] = \"postgres\"\n",
    ") -> str:\n",
    "    match dialect:\n",
    "        case \"postgres\":\n",
    "            dialect = sa.dialects.postgresql.psycopg2.dialect()\n",
    "        case \"oracle\":\n",
    "            dialect = sa.dialects.oracle.cx_oracle.dialect()\n",
    "        case _:\n",
    "            raise ValueError(f\"Unknown dialect {dialect}.\")\n",
    "\n",
    "    statement = str(\n",
    "        statement.compile(dialect=dialect, compile_kwargs={\"literal_binds\": True})\n",
    "    )\n",
    "    return statement\n",
    "\n",
    "\n",
    "def reflect_table(\n",
    "    engine: sa.Engine, table: str, metadata: sa.MetaData | None = None, **kwargs\n",
    ") -> sa.Table:\n",
    "    table = sa.Table(table, metadata or sa.MetaData(), autoload_with=engine, **kwargs)\n",
    "    return table\n",
    "\n",
    "\n",
    "def get_primary_key(table: sa.Table) -> list[str]:\n",
    "    primary_key = table.primary_key.columns.keys()\n",
    "    return primary_key\n",
    "\n",
    "\n",
    "def get_table_datatypes(\n",
    "    table: sa.Table, as_python_dtypes: bool = False, dbapi=None\n",
    ") -> dict:\n",
    "    \"\"\"Dbapi can be the psycopg2 module, for example.\"\"\"\n",
    "    table_datatypes = {}\n",
    "    for column in table.columns:\n",
    "        if as_python_dtypes:\n",
    "            datatype = column.type.python_type\n",
    "        elif dbapi:\n",
    "            datatype = column.type.get_dbapi_type(dbapi)\n",
    "        else:\n",
    "            datatype = column.type\n",
    "\n",
    "        table_datatypes[column.name] = datatype\n",
    "\n",
    "    return table_datatypes\n",
    "\n",
    "\n",
    "def get_table_size(engine, table):\n",
    "    with engine.connect() as connection:\n",
    "        result = connection.execute(\n",
    "            sa.select(sa.func.count()).select_from(sa.text(table))\n",
    "        )\n",
    "        size = result.scalars().first()\n",
    "\n",
    "    return size\n",
    "\n",
    "\n",
    "def is_column_unique(engine, sa_column):\n",
    "    result = execute_query(engine, sa.select(sa.func.count(sa_column), sa.func.count(sa_column.distinct())))\n",
    "    count, distinct_count = result.first()\n",
    "    return count == distinct_count\n",
    "\n",
    "\n",
    "def get_number_of_batches(size, batch_size):\n",
    "    n_batches = (size - 1) // batch_size + 1\n",
    "    return n_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "b9ecc375-9d96-4ed0-9c0b-357694715cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_table(size: int):\n",
    "    df = pd.DataFrame(\n",
    "        {\"id\": range(size), \"x\": np.random.randint(0, 10, size), \"y\": \"string\"}\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def select_all(table):\n",
    "    return f\"SELECT * FROM {table}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "e6e8112b-10fa-4161-8074-8ddac47031d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "database_url = sqlalchemy.URL.create(\n",
    "    drivername=get_postgres_driver_name(),\n",
    "    database=\"learning\",\n",
    ")\n",
    "engine = sqlalchemy.create_engine(database_url)\n",
    "inspector = sa.inspect(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "795c0173-aeab-4f25-9faf-de552f85fe9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with engine.begin() as connection:\n",
    "    connection.execute(sqlalchemy.text(\"DROP TABLE IF EXISTS point;\"))\n",
    "    connection.execute(sqlalchemy.text(\"CREATE TABLE point (x int, y int);\"))\n",
    "    connection.execute(\n",
    "        sqlalchemy.text(\"INSERT INTO point (x, y) VALUES (:x, :y)\"),\n",
    "        [{\"x\": 1, \"y\": 1}, {\"x\": 2, \"y\": 4}],\n",
    "    )\n",
    "\n",
    "with sqlalchemy.orm.Session(engine) as session:\n",
    "    result = session.execute(sqlalchemy.text(\"SELECT * FROM POINT\"))\n",
    "\n",
    "is_column_unique(engine, reflect_table(engine, \"random\").columns[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a547e7-4c60-4d0a-9d73-7eb9ede489b6",
   "metadata": {},
   "source": [
    "### Database inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "99b15612-bc8e-4539-8004-164c8cd9a014",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    inspector.dialect,\n",
    "    inspector.get_schema_names(),\n",
    "    inspector.default_schema_name,\n",
    "    inspector.get_table_names(),\n",
    "    inspector.get_table_oid(\"random\"),\n",
    "    inspector.get_view_definition,\n",
    "    inspector.get_columns(\"random\"),\n",
    "    inspector.get_indexes(\"point\"),\n",
    "    inspector.get_pk_constraint(\"point\"),\n",
    "    inspector.has_table(\"random\", schema=\"public\"),\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a27cbb5-fc77-44f2-9c72-92789cfbdca8",
   "metadata": {},
   "source": [
    "### Vacuum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "473501f4-6709-4d27-aa21-a2529fa1233c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-16 15:34:56,129 INFO sqlalchemy.engine.Engine BEGIN (implicit; DBAPI should not BEGIN due to autocommit mode)\n",
      "2024-06-16 15:34:56,130 INFO sqlalchemy.engine.Engine VACUUM ANALYZE point;\n",
      "2024-06-16 15:34:56,131 INFO sqlalchemy.engine.Engine [generated in 0.00166s] {}\n",
      "2024-06-16 15:34:56,145 INFO sqlalchemy.engine.Engine ROLLBACK using DBAPI connection.rollback(), DBAPI should ignore due to autocommit mode\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.cursor.CursorResult at 0x36aa8c3d0>"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autocommit_engine = to_autocommit_engine(engine)\n",
    "autocommit_engine.echo = True\n",
    "execute_query(autocommit_engine, \"VACUUM ANALYZE point;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e3bf57-70dc-40ab-8ae5-b46d1b5a658f",
   "metadata": {},
   "source": [
    "### Polars_SQL_read_and_write "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "497cf6c3-2026-450d-bc32-8be3053a925e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pl.read_database(select_all(\"point\"), connection=engine)\n",
    "pl.concat([df, df]).write_database(\n",
    "    \"point\", connection=engine.url, if_table_exists=\"append\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da513907-2249-4521-89ab-36337389429f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requested batch size is 2, actual batch size is 2.\n",
      "Note that they are not guaranteed to be equal.\n"
     ]
    }
   ],
   "source": [
    "requested_batch_size = 2\n",
    "for df in pl.read_database(\n",
    "    select_all(\"point\"),\n",
    "    connection=engine,\n",
    "    iter_batches=True,\n",
    "    batch_size=requested_batch_size,\n",
    "    infer_schema_length=0,  # Number of rows to be scanned for schema inference.\n",
    "    schema_overrides={\"x\": pl.Int16, \"y\": int},\n",
    "):\n",
    "    actual_batch_size = len(df)\n",
    "    break\n",
    "\n",
    "print(\n",
    "    f\"Requested batch size is {requested_batch_size}, actual batch size is {actual_batch_size}.\\n\"\n",
    "    f\"Note that they are not guaranteed to be equal.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "96abccdf-5c09-484c-8de7-df3c4bf84222",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = generate_table(10**6)\n",
    "pdf = pl.from_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "61817371-ab10-445a-b5e9-e6aad5b48e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.5 s, sys: 91.4 ms, total: 5.59 s\n",
      "Wall time: 7.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df.to_sql(\"random\", engine, if_exists=\"replace\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "641395a5-e064-4510-a554-8b04b8ca04e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.67 s, sys: 131 ms, total: 5.8 s\n",
      "Wall time: 7.81 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "pdf.write_database(\"random\", engine.url, if_table_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "78534ddf-d5e6-44fd-8c71-e3028ead3f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 970 ms, sys: 31.8 ms, total: 1 s\n",
      "Wall time: 1.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for _ in pd.read_sql(select_all(\"random\"), engine, chunksize=10**3):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "1f23a3cd-8230-4838-93f0-25281916169f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 572 ms, sys: 24.6 ms, total: 597 ms\n",
      "Wall time: 697 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for _ in pl.read_database(\n",
    "    select_all(\"random\"), engine, iter_batches=True, batch_size=10**3\n",
    "):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5355b3-fd46-4717-ac0c-5fb8a42bd6a3",
   "metadata": {},
   "source": [
    "### Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cb545884-4f54-4ea8-8071-85537f879993",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = sqlalchemy.MetaData()\n",
    "table = sqlalchemy.Table(\"point\", metadata, schema=\"public\", autoload_with=engine)\n",
    "\n",
    "\"point\" in metadata.tables\n",
    "table.indexes, table.primary_key, table.foreign_keys;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95ea846-36f3-41ad-b25d-2d64cde8f645",
   "metadata": {},
   "source": [
    "### ORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82a5797b-28eb-4557-9dca-a75baee1825e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(MetaData(), <sqlalchemy.orm.attributes.InstrumentedAttribute at 0x12e929080>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Base(sqlalchemy.orm.DeclarativeBase):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Point(Base):\n",
    "    __tablename__ = \"point\"\n",
    "\n",
    "    id: sqlalchemy.orm.Mapped[int] = sqlalchemy.orm.mapped_column(primary_key=True)\n",
    "    x: sqlalchemy.orm.Mapped[int]\n",
    "    y: sqlalchemy.orm.Mapped[int]\n",
    "\n",
    "\n",
    "Point.metadata, Point.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "95401ce2-d268-40b5-b154-7541a027b3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSERT INTO public.point (x, y) VALUES (:x, :y)\n",
      "INSERT INTO public.point (x, y) VALUES (%(x)s, %(y)s)\n",
      "{'x': 3, 'y': 5}\n"
     ]
    }
   ],
   "source": [
    "statement = sqlalchemy.insert(table).values(x=3, y=5)\n",
    "print(statement, statement.compile(engine), statement.compile().params, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e778c27f-4432-4ea7-beba-65d3b075e382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implicitly parametrizes insert statement\n",
    "\n",
    "with engine.begin() as connection:\n",
    "    result = connection.execute(\n",
    "        sqlalchemy.insert(table),\n",
    "        [\n",
    "            {\"x\": 2, \"y\": 7},\n",
    "            {\"x\": -1, \"y\": 5},\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "244a916e-a134-46f6-92db-5a3310702aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT public.point.x, public.point.y \n",
      "FROM public.point \n",
      "WHERE public.point.x > :x_1\n"
     ]
    }
   ],
   "source": [
    "print(sqlalchemy.select(table).where(table.c.x > 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "65d15b62-4067-41a4-bf21-07f65a7cb45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT point.id, point.x, point.y \n",
      "FROM point \n",
      "WHERE point.x > :x_1\n"
     ]
    }
   ],
   "source": [
    "print(sqlalchemy.select(Point).where(Point.x > 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "1478193e-3e86-4fe7-a8b6-93d62e699019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT point_1.x, public.point.y \n",
      "FROM point AS point_1, public.point\n"
     ]
    }
   ],
   "source": [
    "print(sqlalchemy.select(Point.x, table.c[\"y\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "9a7249e0-b90c-4ef1-a38c-71562529cc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT random.id + :id_1 AS random_id \n",
      "FROM random \n",
      "WHERE random.y = :y_1 ORDER BY random.x\n"
     ]
    }
   ],
   "source": [
    "random = sqlalchemy.Table(\"random\", metadata, autoload_with=engine)\n",
    "print(\n",
    "    sa.select((random.c.id + 1).label(\"random_id\"))\n",
    "    .where(random.c.y == 7)\n",
    "    .order_by(random.c.x)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553e7dcc-bfb0-481f-82fd-b52aeb7bf2d8",
   "metadata": {},
   "source": [
    "### Read_table_in_parallel_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01779134-88be-405c-abf3-a1d0a08e2637",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "            WITH tiled_primary_keys AS (\n",
    "                SELECT \n",
    "                    {primary_key}, \n",
    "                    (ntile({self.n_batches}) OVER (ORDER BY {primary_key})) AS tile\n",
    "                FROM {table}\n",
    "            )\n",
    "            SELECT DISTINCT ON(tile) {primary_key}\n",
    "            FROM tiled_primary_keys\n",
    "            ORDER BY tile, {primary_key}\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "701c4a41-56db-40de-8518-2bcf576897cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndependentBatchReader:\n",
    "    def __init__(\n",
    "        self,\n",
    "        database_url: str,\n",
    "        table: str,\n",
    "        primary_key: str,\n",
    "        batch_size: int,\n",
    "        database_backend: Literal[\"pandas\", \"polars\"] = \"pandas\",\n",
    "    ):\n",
    "        self.database_url = database_url\n",
    "        engine = sa.create_engine(database_url)\n",
    "        self.table = reflect_table(engine, table)\n",
    "        self.primary_key = primary_key\n",
    "        self.batch_size = batch_size\n",
    "        self.database_backend = database_backend\n",
    "        self.size = get_table_size(engine, table)\n",
    "        self.n_batches = get_number_of_batches(self.size, batch_size)\n",
    "\n",
    "    def query_to_dataframe(self, query):\n",
    "        match self.database_backend:\n",
    "            case \"pandas\":\n",
    "                df = pd.read_sql(query, con=self.database_url)\n",
    "            case \"polars\":\n",
    "                engine = sa.create_engine(self.database_url)\n",
    "                table_datatypes = get_table_datatypes(self.table, as_python_dtypes=True)\n",
    "                df = pl.read_database(\n",
    "                    query,\n",
    "                    connection=engine,\n",
    "                    schema_overrides=table_datatypes,\n",
    "                    infer_schema_length=0,\n",
    "                )\n",
    "            case _:\n",
    "                raise ValueError(f\"Unknown database backend '{database_backend}'.\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def offset_read(self, batch_begin):\n",
    "        query = (\n",
    "            self.table.select()\n",
    "            .order_by(self.primary_key)\n",
    "            .offset(batch_begin)\n",
    "            .limit(self.batch_size)\n",
    "        )\n",
    "        df = self.query_to_dataframe(query)\n",
    "        return df\n",
    "\n",
    "    def primary_key_read(self, batch_begin, batch_end=None):\n",
    "        if batch_end:\n",
    "            where = sa.between(sa.column(self.primary_key), batch_begin, batch_end)\n",
    "        else:\n",
    "            where = sa.column(self.primary_key) >= batch_begin\n",
    "\n",
    "        query = self.table.select().where(where)\n",
    "        df = self.query_to_dataframe(query)\n",
    "        return df\n",
    "\n",
    "    def generate_offset_reads(self):\n",
    "        \"\"\"Simple, but recomputes all preceding rows for each batch.\"\"\"\n",
    "        for batch_index in range(self.n_batches):\n",
    "            yield functools.partial(\n",
    "                self.offset_read, batch_begin=batch_index * self.batch_size\n",
    "            )\n",
    "\n",
    "    def build_primary_key_begins_query(self):\n",
    "        \"\"\"\n",
    "        WITH tiled_primary_keys AS (\n",
    "            SELECT\n",
    "                {primary_key},\n",
    "                (ntile({self.n_batches}) OVER (ORDER BY {primary_key})) AS tile\n",
    "            FROM {table}\n",
    "        )\n",
    "        SELECT DISTINCT ON(tile) {primary_key}\n",
    "        FROM tiled_primary_keys\n",
    "        ORDER BY tile, {primary_key}\n",
    "        \"\"\"\n",
    "        pk = sa.column(self.primary_key)\n",
    "        cte = (\n",
    "            sa.select(pk, sa.func.ntile(self.n_batches).over(order_by=pk).label(\"tile\"))\n",
    "            .select_from(self.table)\n",
    "            .cte(\"tiled_primary_keys\")\n",
    "        )\n",
    "        query = sa.select(pk).distinct(\"tile\").select_from(cte).order_by(\"tile\", pk)\n",
    "        return query\n",
    "\n",
    "    def generate_primary_key_reads(self):\n",
    "        \"\"\"Efficient, but more complex.\"\"\"\n",
    "        query = self.build_primary_key_begins_query()\n",
    "        with sa.create_engine(self.database_url).connect() as connection:\n",
    "            result = connection.execute(query)\n",
    "            primary_key_begins = list(result.scalars())\n",
    "\n",
    "        for batch_index, batch_begin in enumerate(primary_key_begins):\n",
    "            if batch_index + 1 < len(primary_key_begins):\n",
    "                batch_end = primary_key_begins[batch_index + 1] - 1\n",
    "            else:\n",
    "                batch_end = None\n",
    "\n",
    "            yield functools.partial(\n",
    "                self.primary_key_read,\n",
    "                batch_begin=batch_begin,\n",
    "                batch_end=batch_end,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "7b39416d-7588-4485-ad9c-56b8637f879e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PrimaryKeyConstraint()"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle table\n",
    "table = \"random\"\n",
    "primary_key = \"id\"\n",
    "table_df = generate_table(10**6)\n",
    "table_df = table_df.sample(frac=1)\n",
    "table_df.to_sql(table, engine, if_exists=\"replace\", index=False)\n",
    "\n",
    "### No primary key\n",
    "random = sqlalchemy.Table(table, sqlalchemy.MetaData(), autoload_with=engine)\n",
    "random.primary_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "76fd92cd-9301-4bab-8e4b-4df10e54ed68",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10**4\n",
    "n_jobs = -1\n",
    "reader = IndependentBatchReader(\n",
    "    str(engine.url), table, primary_key, batch_size, database_backend=\"pandas\"\n",
    ")\n",
    "parallel = joblib.Parallel(n_jobs=n_jobs, return_as=\"generator_unordered\")\n",
    "\n",
    "\n",
    "def time_read_function(read_function):\n",
    "    def f():\n",
    "        pks = []\n",
    "        for df in parallel(joblib.delayed(i)() for i in read_function()):\n",
    "            assert len(df) == reader.batch_size\n",
    "            pks.extend(list(df[primary_key]))\n",
    "\n",
    "        assert len(set(pks)) == reader.size\n",
    "\n",
    "    %time f()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "105dd0d0-58f3-44b5-9d5b-7e6b37fb813c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 698 ms, sys: 165 ms, total: 863 ms\n",
      "Wall time: 7.03 s\n",
      "CPU times: user 482 ms, sys: 114 ms, total: 596 ms\n",
      "Wall time: 2 s\n"
     ]
    }
   ],
   "source": [
    "time_read_function(reader.generate_offset_reads)\n",
    "time_read_function(reader.generate_primary_key_reads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "2b1e76c2-29e3-4cff-a6b4-cef863df1772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 612 ms, sys: 131 ms, total: 743 ms\n",
      "Wall time: 3.24 s\n",
      "CPU times: user 405 ms, sys: 89.9 ms, total: 495 ms\n",
      "Wall time: 1.42 s\n"
     ]
    }
   ],
   "source": [
    "with engine.begin() as connection:\n",
    "    connection.execute(\n",
    "        sqlalchemy.text(f\"ALTER TABLE {table} ADD PRIMARY KEY ({primary_key})\")\n",
    "    )\n",
    "\n",
    "time_read_function(reader.generate_offset_reads)\n",
    "time_read_function(reader.generate_primary_key_reads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "d46887d5-e3ff-4d9d-96f4-f08a204e39a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 605 ms, sys: 114 ms, total: 719 ms\n",
      "Wall time: 3.39 s\n",
      "CPU times: user 397 ms, sys: 99.2 ms, total: 496 ms\n",
      "Wall time: 1.02 s\n"
     ]
    }
   ],
   "source": [
    "execute_query(to_autocommit_engine(engine), f\"VACUUM ANALYZE {table}\")\n",
    "time_read_function(reader.generate_offset_reads)\n",
    "time_read_function(reader.generate_primary_key_reads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb9a7fb-f21a-4f25-92f0-560119a4c37a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
