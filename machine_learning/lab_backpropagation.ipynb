{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "UrkSsCyT5CoR"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ObuqtNmF5CoW"
   },
   "source": [
    "**Module** is an abstract class which defines fundamental methods necessary for a training a neural network. You do not need to change anything here, just read the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "9-RImaVZ5CoW"
   },
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    \"\"\"\n",
    "    Basically, you can think of a module as of a something (black box)\n",
    "    which can process `input` data and produce `ouput` data.\n",
    "    This is like applying a function which is called `forward`:\n",
    "\n",
    "        output = module.forward(input)\n",
    "\n",
    "    The module should be able to perform a backward pass: to differentiate the `forward` function.\n",
    "    Moreover, it should be able to differentiate it if is a part of chain (chain rule).\n",
    "    The latter implies there is a gradient from previous step of a chain rule.\n",
    "\n",
    "        input_grad = module.backward(input, output_grad)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._output = None\n",
    "        self._input_grad = None\n",
    "        self.training = True\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Takes an input object, and computes the corresponding output of the module.\n",
    "        \"\"\"\n",
    "        self._output = self._compute_output(input)\n",
    "        return self._output\n",
    "\n",
    "    def backward(self, input, output_grad):\n",
    "        \"\"\"\n",
    "        Performs a backpropagation step through the module, with respect to the given input.\n",
    "\n",
    "        This includes\n",
    "         - computing a gradient w.r.t. `input` (is needed for further backprop),\n",
    "         - computing a gradient w.r.t. parameters (to update parameters while optimizing).\n",
    "        \"\"\"\n",
    "        self._input_grad = self._compute_input_grad(input, output_grad)\n",
    "        self._update_parameters_grad(input, output_grad)\n",
    "        return self._input_grad\n",
    "\n",
    "    def _compute_output(self, input):\n",
    "        \"\"\"\n",
    "        Computes the output using the current parameter set of the class and input.\n",
    "        This function returns the result which will be stored in the `_output` field.\n",
    "\n",
    "        Example: in case of identity operation:\n",
    "\n",
    "        output = input\n",
    "        return output\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _compute_input_grad(self, input, output_grad):\n",
    "        \"\"\"\n",
    "        Returns the gradient of the module with respect to its own input.\n",
    "        The shape of the returned value is always the same as the shape of `input`.\n",
    "\n",
    "        Example: in case of identity operation:\n",
    "        input_grad = output_grad\n",
    "        return input_grad\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _update_parameters_grad(self, input, output_grad):\n",
    "        \"\"\"\n",
    "        Computing the gradient of the module with respect to its own parameters.\n",
    "        No need to override if module has no parameters (e.g. ReLU).\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\"\n",
    "        Zeroes `gradParams` variable if the module has params.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def get_parameters(self):\n",
    "        \"\"\"\n",
    "        Returns a list with its parameters.\n",
    "        If the module does not have parameters return empty list.\n",
    "        \"\"\"\n",
    "        return []\n",
    "\n",
    "    def get_parameters_grad(self):\n",
    "        \"\"\"\n",
    "        Returns a list with gradients with respect to its parameters.\n",
    "        If the module does not have parameters return empty list.\n",
    "        \"\"\"\n",
    "        return []\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Sets training mode for the module.\n",
    "        Training and testing behaviour differs for Dropout, BatchNorm.\n",
    "        \"\"\"\n",
    "        self.training = True\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Sets evaluation mode for the module.\n",
    "        Training and testing behaviour differs for Dropout, BatchNorm.\n",
    "        \"\"\"\n",
    "        self.training = False\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Pretty printing. Should be overrided in every module if you want\n",
    "        to have readable description.\n",
    "        \"\"\"\n",
    "        return \"Module\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "isbDKqDW5Col"
   },
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybvPD8p05Coq"
   },
   "source": [
    "## 1. Batch normalization\n",
    "One of the most significant recent ideas that impacted NNs a lot is [**Batch normalization**](http://arxiv.org/abs/1502.03167). The idea is simple, yet effective: the features should be whitened ($mean = 0$, $std = 1$) all the way through NN. This improves the convergence for deep models letting it train them for days but not weeks. **You are** to implement the first part of the layer: features normalization. The second part (`ChannelwiseScaling` layer) is implemented below.\n",
    "\n",
    "- input:   **`batch_size x n_feats`**\n",
    "- output: **`batch_size x n_feats`**\n",
    "\n",
    "The layer should work as follows. While training (`self.training == True`) it transforms input as $$y = \\frac{x - \\mu}  {\\sqrt{\\sigma + \\epsilon}}$$\n",
    "where $\\mu$ and $\\sigma$ â€” mean and variance of feature values in **batch** and $\\epsilon$ is just a small number for numericall stability. Also during training, layer should maintain exponential moving average values for mean and variance: \n",
    "```\n",
    "    self.moving_mean = self.moving_mean * alpha + batch_mean * (1 - alpha)\n",
    "    self.moving_variance = self.moving_variance * alpha + batch_variance * (1 - alpha)\n",
    "```\n",
    "During testing (`self.training == False`) the layer normalizes input using moving_mean and moving_variance. \n",
    "\n",
    "Note that decomposition of batch normalization on normalization itself and channelwise scaling here is just a common **implementation** choice. In general \"batch normalization\" always assumes normalization + scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "Let's walk through the process of backpropagation. Given a composition of functions \n",
    "$L_n \\equiv L \\circ f_1 \\circ \\dots \\circ f_n$, \n",
    "$f_i : \\mathbb{R}^{d_{i}} \\rightarrow \\mathbb{R}^{d_{i - 1}},\n",
    "L :  \\mathbb{R}^{d_0} \\rightarrow \\mathbb{R}$ and a point $x_n \\in \\mathbb{R}^{d_n}$, we want to find the update value $\\Delta x_n$, such that composition value decreases: $L_n(x_n + \\Delta x_n) < L_n(x_n)$. Notice that in this loose formulation, the task isn't fully determined. The natural way to make it determined is to place a restriction on the length of $\\Delta x_n$ and ask for largest loss decrease. For now let's keep the freedom to choose the exact nature of the restriction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we know the value of gradient $\\nabla L_n$ at point $x_n$. Then $\\Delta L \\equiv L_n(x_n+\\Delta x_n) - L_n(x_n) \\approx \\nabla L_n(x_n)\\Delta x_n$. If we assume this approximation to be an equality, then $\\Delta x = -\\nabla L_n(x) \\epsilon$ will be the vector that leads to largest (approximate) loss decrease among vectors with same length. This kind of restriction is based mostly on computational convenience, and will affect the loss in a complex way: $\\Delta L \\approx -\\nabla L_n^2(x_n) \\epsilon$. Considering that in practice all vectors get updated simultaneously, more realistic approximation is this: $\\Delta L \\approx -\\epsilon\\sum_n\\nabla L_n^2(x_n)$. The quadratic dependence on gradient values can make the gradient descent unstable in some cases and too slow in others, and often requires some heuristics to improve it's performance, like gradient clipping and momentum. Also it's intuitive to choose $\\epsilon = \\frac{1}{|\\nabla L_n|}$ to achieve more stable linear gradient descent, which is basically what the Adam learning rate scheduler does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, but how do we find $\\nabla L_n(x_n)$ for each n? Let's use the chain rule: \n",
    "\n",
    "$$\\nabla L_{n}(x_{n}) = \\nabla (L_{n-1} \\circ f_n) (x_{n}) = J_n^T(x_n) \\cdot \\nabla L_{n-1}(f_{n}(x_n))=\n",
    "J_n^T(x_n) \\cdot \\nabla L_{n-1}(x_{n-1}):$$\n",
    "\n",
    "$$ \\nabla (L_{n-1} \\circ f_n)(x_n)_i = \n",
    "\\cfrac{\\partial (L_{n-1} \\circ f_n)}{\\partial x_n^i}(x_n) = \n",
    "\\sum_{j\\in 1\\dots d_{n-1}} \\cfrac{\\partial L_{n-1}}{\\partial x_{n-1}^j}(f_n(x_n)) \\cfrac{\\partial f_n^j}{\\partial x_n^i}(x_n)=\n",
    "\\sum_{j\\in 1\\dots d_{n-1}} \\nabla L_{n-1}(f_{n}(x_n)) J_n^{ji}(x_n)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, knowing $\\nabla L_{n-1}(x_{n-1})$ and jacobian $J_n$ at point $x_n$, we can calculate $\\nabla L_{n}(x_{n})$. To complete the induction, we need its base and a way to calculate $J_n(x_n)$. Let's assign fictive $\\nabla L_{-1}(x_{-1}) = 1$, then $\\nabla L_{0}(x_{0}) = J_0^T(x_0) \\cdot \\nabla L_{-1}(x_{-1}) = J_0^T(x_0) = \\nabla L(x_0)$. So, everything came down to jacobians. To find the points $x_n$ at which to calculate the jacobians, we'll need to do a so-called forward pass through the composition chain. Then, using functions with closed-form partial derivatives, or a lookup table, calculate the jacobians. Here we can notice that technically we don't need to know the loss value $L(x_0)$, but only its gradient at any point. This can be usefull when we know the gradient and don't want to solve the differential equation to find the expression for the loss function (which may not even be possible), for example in recommending tasks where we can compare two objects, but not assign an absolute value to each one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batchnorm output:\n",
    "\n",
    "$$f: \\mathbb{R}^{B\\times N}\\rightarrow \\mathbb{R}^{B\\times N},\n",
    "f(x) = \\cfrac{x - \\mu(x)}{\\sqrt{\\text{Var}(x) + \\epsilon}},\\,\n",
    "f(x)_{bn} = \\cfrac{x_{bn} - \\mu(x)_n}{\\sqrt{\\text{Var}(x)_n + \\epsilon}},$$\n",
    "\n",
    "$$\\mu(x)_{n} = \\frac{\\sum_i x_{in}}{B},\\\\\n",
    "\\cfrac{\\partial \\mu}{\\partial x_{b_1n_1}}(x)_{n_0} = \\cfrac{[n_0 == n_1]}{B}$$\n",
    "\n",
    "$$\n",
    "\\text{Var}(x)_{n} = \\frac{\\sum_i (x_{in} - \\mu(x)_{n})^2}{B},\\\\\n",
    "\\cfrac{\\partial \\text{Var}}{\\partial x_{b_1n_1}}(x)_{n_0} = \n",
    "\\cfrac{\\sum_i 2\\bigl([i == b_1 \\land n_0 == n_1] - \\cfrac{[n_0 == n_1]}{B}\\bigr)\n",
    "\\bigl(x_{i n_0} - \\mu(x)_{n_0} \\bigr)}{B}= \\\\\n",
    " = \\cfrac{2[n_0 == n_1]}{B}\n",
    "\\bigl(x_{b_1n_0} - \\mu(x)_{n_0} \\bigr) +\n",
    "\\cfrac{2[ n_0 == n_1]}{B^2}\n",
    "\\bigl( B \\mu(x)_{n_0}  - \\sum_i x_{i n_0} \\bigr) = \n",
    "\\cfrac{2[ n_0 == n_1]}{B}\n",
    "\\bigl(x_{b_1n_0} - \\mu(x)_{n_0} \\bigr)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\nabla \\hat L$ be the output gradient, then batchnorm gradient $\\nabla L$ is expressed in this way:\n",
    " \n",
    "$$ \\nabla L(x)_{ij} = \n",
    "\\sum_{b \\in 1\\dots B, \\, n\\in 1\\dots N} \\nabla \\hat L_{bn}(f(x)) \\cfrac{\\partial f_{bn}}{\\partial x_{ij}}(x)=\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "= \\sum_{b, \\, n}\n",
    "\\cfrac{\\bigl(\n",
    "\\cfrac{\\partial x_{b n}}{\\partial x_{i j}}(x) - \\cfrac{\\partial \\mu}{\\partial x_{ij}}(x)_{ n}\n",
    "\\bigr)\\sqrt{\\text{Var}(x)_{n} + \\epsilon} - \n",
    "(x_{bn} - \\mu(x)_{ n})\\cfrac{1}{2\\sqrt{\\text{Var}(x)_{n} + \\epsilon}}\\cfrac{\\partial \\text{Var}}{\\partial x_{ij}}(x)_{n}}\n",
    "{\\text{Var}(x)_{n} + \\epsilon}\n",
    "\\nabla \\hat L_{bn} = \\\\\n",
    "= \\sum_{b, \\, n}\n",
    "\\cfrac{\\bigl([b == i \\land n == j] - [n == j]\\cfrac{1}{B}\\bigr)(\\text{Var}(x)_{n} + \\epsilon) - (x_{b n} - \\mu(x)_{ n})\\cfrac{[n == j]}{B}\n",
    "\\bigl(x_{i n} - \\mu(x)_{n} \\bigr)}\n",
    "{\\sqrt{\\text{Var}(x)_{n} + \\epsilon}^3}\n",
    "\\nabla \\hat L_{bn} = \\\\\n",
    "= \\sum_{b}\n",
    "\\cfrac{[i == b] - \\cfrac{1}{B} }\n",
    "{\\sqrt{\\text{Var}(x)_{j} + \\epsilon}}\n",
    "\\nabla \\hat L_{bj} -\n",
    "\\cfrac{(x_{i j} - \\mu(x)_{ j})\n",
    " }\n",
    "{B\\sqrt{\\text{Var}(x)_{j} + \\epsilon}^3}\n",
    "\\sum_{b}(x_{b j} - \\mu(x)_{j})\n",
    "\\nabla \\hat L_{bj}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let:\n",
    "\n",
    "$\\text{output_grad} = \\nabla \\hat L\\\\\n",
    "\\text{inverse_std} = \\cfrac{1}{\\sqrt{\\text{Var}(x) + \\epsilon}}\\\\\n",
    "\\text{eye_minus_inverse_b} = E - \\cfrac{1}{B}\\\\\n",
    "\\text{centered_input} = x - \\mu(x)\n",
    "$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\\text{grad_input}_{ij} =\n",
    "\\sum_b \\text{eye_minus_inverse_b}_{ib} \\cdot \\text{inverse_std}_{j} \\cdot \\text{output_grad}_{bj} +\\\\\n",
    "-\\cfrac{1}{B}\\sum_b \\text{centered_input}_{ij} \\cdot\\text{centered_input}_{bj} \\cdot \\text{inverse_std}^3_{j} \\cdot \\text{output_grad}_{bj} := \\text{first} -\\cfrac{1}{B}\\text{second}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "XQRFBjo55Cos"
   },
   "outputs": [],
   "source": [
    "class BatchNormalization(Module):\n",
    "    EPS = 1e-3\n",
    "\n",
    "    def __init__(self, alpha=0.0):\n",
    "        super(BatchNormalization, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.moving_mean = 0.0\n",
    "        self.moving_variance = 1.0\n",
    "\n",
    "    def _compute_output(self, input):\n",
    "        if self.training == False:\n",
    "            output = (input - self.moving_mean) / (\n",
    "                self.moving_variance + self.EPS\n",
    "            ) ** 0.5\n",
    "            return output\n",
    "\n",
    "        mean = input.mean(axis=0)\n",
    "        var = input.var(axis=0)\n",
    "        output = (input - mean) / (var + self.EPS) ** 0.5\n",
    "\n",
    "        self.moving_mean = self.moving_mean * self.alpha + mean * (1 - self.alpha)\n",
    "        self.moving_variance = self.moving_variance * self.alpha + var * (\n",
    "            1 - self.alpha\n",
    "        )\n",
    "\n",
    "        return output\n",
    "\n",
    "    def _compute_input_grad(self, input, output_grad):\n",
    "        inverse_std = (input.var(axis=0) + self.EPS) ** (-1 / 2)\n",
    "        B, N = input.shape\n",
    "        eye_minus_inverse_b = np.eye(B) - 1 / B\n",
    "        centered_input = input - input.mean(axis=0)\n",
    "\n",
    "        first = np.einsum(\n",
    "            \"ib, j, bj -> ij\", eye_minus_inverse_b, inverse_std, output_grad\n",
    "        )\n",
    "        second = np.einsum(\n",
    "            \"ij, bj, j, bj -> ij\",\n",
    "            centered_input,\n",
    "            centered_input,\n",
    "            inverse_std**3,\n",
    "            output_grad,\n",
    "        )\n",
    "\n",
    "        grad_input = first - 1 / B * second\n",
    "\n",
    "        return grad_input\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"BatchNormalization\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "iU8LwKVy5Cot"
   },
   "outputs": [],
   "source": [
    "class ChannelwiseScaling(Module):\n",
    "    \"\"\"\n",
    "    Implements linear transform of input y = \\gamma * x + \\beta\n",
    "    where \\gamma, \\beta - learnable vectors of length x.shape[-1]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_out):\n",
    "        super(ChannelwiseScaling, self).__init__()\n",
    "\n",
    "        stdv = 1.0 / np.sqrt(n_out)\n",
    "        self.gamma = np.random.uniform(-stdv, stdv, size=n_out)\n",
    "        self.beta = np.random.uniform(-stdv, stdv, size=n_out)\n",
    "\n",
    "        self.gradGamma = np.zeros_like(self.gamma)\n",
    "        self.gradBeta = np.zeros_like(self.beta)\n",
    "\n",
    "    def _compute_output(self, input):\n",
    "        output = input * self.gamma + self.beta\n",
    "        return output\n",
    "\n",
    "    def _compute_input_grad(self, input, output_grad):\n",
    "        grad_input = output_grad * self.gamma\n",
    "        return grad_input\n",
    "\n",
    "    def _update_parameters_grad(self, input, output_grad):\n",
    "        self.gradBeta = np.sum(output_grad, axis=0)\n",
    "        self.gradGamma = np.sum(output_grad * input, axis=0)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.gradGamma.fill(0)\n",
    "        self.gradBeta.fill(0)\n",
    "\n",
    "    def get_parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "    def get_parameters_grad(self):\n",
    "        return [self.gradGamma, self.gradBeta]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"ChannelwiseScaling\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDIWs4955Cou"
   },
   "source": [
    "Practical notes. If BatchNormalization is placed after a linear transformation layer (including dense layer, convolutions, channelwise scaling) that implements function like `y = weight * x + bias`, than bias adding become useless and could be omitted since its effect will be discarded while batch mean subtraction. If BatchNormalization (followed by `ChannelwiseScaling`) is placed before a layer that propagates scale (including ReLU, LeakyReLU) followed by any linear transformation layer than parameter `gamma` in `ChannelwiseScaling` could be freezed since it could be absorbed into the linear transformation layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9Wcfkiu5Cov"
   },
   "source": [
    "## 2. Dropout\n",
    "Implement [**dropout**](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf). The idea and implementation is really simple: just multimply the input by $Bernoulli(p)$ mask. Here $p$ is probability of an element to be zeroed.\n",
    "\n",
    "This has proven to be an effective technique for regularization and preventing the co-adaptation of neurons.\n",
    "\n",
    "While training (`self.training == True`) it should sample a mask on each iteration (for every batch), zero out elements and multiply elements by $1 / (1 - p)$. The latter is needed for keeping mean values of features close to mean values which will be in test mode. When testing this module should implement identity transform i.e. `output = input`.\n",
    "\n",
    "- input:   **`batch_size x n_feats`**\n",
    "- output: **`batch_size x n_feats`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout output:\n",
    "\n",
    "$$f: \\mathbb{R}^{B\\times N}\\rightarrow \\mathbb{R}^{B\\times N},\n",
    "f(x) = \\cfrac{\\text{mask}}{1-p}x,\\,f(x)_{bn} = \\cfrac{\\text{mask}_{bn}}{1-p}x_{bn},\\,\n",
    "\\text{mask}_{bn}\\sim \\text{Bernoulli}(p)$$\n",
    "\n",
    "Dropout gradient:\n",
    "\n",
    "$$ \\nabla L(x)_{ij} = \n",
    "\\sum_{b \\in 1\\dots B, \\, n\\in 1\\dots N}  \\cfrac{\\partial f_{bn}}{\\partial x_{ij}}(x) \\nabla \\hat L_{bn}=\n",
    "\\sum_{bn}\n",
    "\\cfrac{\\text{mask}_{bn}}{1-p}\n",
    "\\cfrac{\\partial x_{bn}}{\\partial x_{ij}} (x)\n",
    "\\nabla \\hat L_{bn}=\n",
    "\\cfrac{\\text{mask}_{ij}}{1-p}\n",
    "\\nabla \\hat L_{ij}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "AxnN67MZ5Cow"
   },
   "outputs": [],
   "source": [
    "class Dropout(Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super(Dropout, self).__init__()\n",
    "\n",
    "        self.p = p\n",
    "        self.mask = []\n",
    "\n",
    "    def _compute_output(self, input):\n",
    "        if self.training == False:\n",
    "            return input\n",
    "\n",
    "        self.mask = 1 - np.random.binomial(1, self.p, input.shape)\n",
    "        output = 1 / (1 - self.p) * self.mask * input\n",
    "\n",
    "        return output\n",
    "\n",
    "    def _compute_input_grad(self, input, output_grad):\n",
    "        grad_input = 1 / (1 - self.p) * self.mask * output_grad\n",
    "\n",
    "        return grad_input\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Dropout\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Conv2d\n",
    "\n",
    "* input: `batch_size x in_channels x h x w`\n",
    "* output: `batch_size x out_channels x h x w`\n",
    "\n",
    "You should implement something like pytorch Conv2d layer with `stride=1` and zero-padding outside of image using `scipy.signal.correlate` function.\n",
    "\n",
    "**Practical notes:**\n",
    "\n",
    "* While the layer name is \"convolution\", the most of neural network frameworks (including tensorflow and pytorch) implement operation that is called [correlation](https://en.wikipedia.org/wiki/Cross-correlation#Cross-correlation_of_deterministic_signals) in signal processing theory. **So don't use** `scipy.signal.convolve` since it implements convolution in terms of signal processing.\n",
    "* It may be convenient to use numpy.pad for zero-padding.\n",
    "* It's rather ok to implement convolution over 4d array using 2 nested loops: one over batch size dimension and another one over output filters dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conv2d output:\n",
    "\n",
    "$$f: \n",
    "\\mathbb{R}^{B\\times I\\times H \\times W}\\rightarrow \n",
    "\\mathbb{R}^{B\\times O\\times (H + 2P + 1 - K) \\times (W + 2P + 1 - K)},\n",
    "P - \\text{padding}, K - \\text{kernel size},$$\n",
    "\n",
    "$$\n",
    "f(x)_{bohw} = \\text{bias}_{o} + \\text{conv2d}(x)_{bohw},$$\n",
    "\n",
    "$$\n",
    "\\text{conv2d}(x;kernel,p)_{bohw} = \\sum_{i\\in0\\dots I-1,\\, h_k\\in 0\\dots K -1,\\, w_k \\in 0\\dots K - 1} \n",
    "\\text{padded}(x, p)_{b,i,h + h_k, w+w_k} \\cdot \\text{kernel}_{o i h_k w_k},$$\n",
    "\n",
    "$$\n",
    "\\text{padded}(x,p)_{b,i,h,w} = [h \\in [p, H_x + p - 1] \\land w \\in [p, W_x + p - 1]] \\cdot x_{b, i, h - p, w-p} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conv2d gradient:\n",
    "\n",
    "$$\\nabla L(x)_{b_xi_xh_xw_x} =\n",
    "\\sum_{\n",
    "b \\in 0\\dots B-1, \\, \n",
    "o \\in 0\\dots O-1,\\,\n",
    "h \\in 0\\dots H+2P -K,\\,\n",
    "w \\in 0\\dots W+2P -K\n",
    "}\n",
    "\\cfrac{\\partial f}{\\partial x_{b_xi_xh_xw_x}} (x)_{bohw}\n",
    "\\nabla \\hat L_{bohw} = \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$=\n",
    "\\sum_{b,\\,o,\\,h,\\,w}\\,\n",
    "\\sum_{i,\\,h_k,\\,w_k}\n",
    "\\cfrac{\\partial\\text{padded}}{\\partial x_{b_xi_xh_xw_x}}(x, p)_{b,i,h+h_k,w + w_k} \\cdot \\text{kernel}_{o i h_k w_k}\n",
    "\\nabla \\hat L_{bohw} =\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$=\n",
    "\\sum_{b,\\,o,\\,h,\\,w}\\,\n",
    "\\sum_{i,\\,h_k,\\,w_k}\n",
    "[h+h_k \\in [p, H_x + p - 1] \\land w + w_k \\in [p, W_x + p - 1]] \\cdot \n",
    "[b == b_x \\land i==i_x \\land h + h_k-p == h_x \\land w + w_k-p == w_x] \\cdot \\\\\n",
    "\\cdot \n",
    "\\text{kernel}_{o i h_k w_k}\n",
    "\\nabla \\hat L_{bohw}\n",
    "$$\n",
    "\n",
    "Let's replace\n",
    "$$b = b_x,\\,i = i_x,\\,h = h_x - h_k + p,\\,w = w_x - w_k + p: $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla L(x)_{b_xi_xh_xw_x}= \n",
    "\\sum_{o,\\,h_k,\\,w_k}\n",
    "[h_x + p \\in [p, H_x + p - 1] \\land w_x + p \\in [p, W_x + p - 1]] \\cdot \n",
    "\\text{kernel}_{o_x, i, h_k, w_k}\n",
    "\\nabla \\hat L_{b,o, h_x - h_k + p, w_x - w_k + p} =\\\\\n",
    "=\\sum_{o,\\,h_k = 0\\dots K-1,\\,w_k=0\\dots K-1}\n",
    "\\text{padded}(\\nabla \\hat L, p)_{b,o, h_x - h_k + 2p, w_x - w_k + 2p} \\cdot \n",
    "\\text{kernel}_{o_x, i, h_k, w_k}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, that if $K = 2p+1$, then $h'_k \\equiv 2p-h_k=K-1 - h_k$ and $w'_k \\equiv 2p-w_k=K-1 - w_k$ also take values in the range $0\\dots K-1$, and the gradient is equal to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla L(x)_{b_xi_xh_xw_x}= \n",
    "\\sum_{o,\\,h'_k = 0\\dots K-1,\\,w'_k=0\\dots K-1}\n",
    "\\text{padded}(\\nabla \\hat L, p)_{b,o, h_x + h_k', w_x + w_k'} \\cdot \n",
    "\\text{kernel}_{o_x, i, 2p - h_k', 2p-w_k'}=\n",
    "\\text{conv2d}(\\nabla \\hat L;kernel',p)_{b_xi_xh_xw_x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $kernel'_{oihw} = kernel_{i,o,K -1- h, L-1-w}$, i.e. it is equal to original kernel with transposed first two dimensions and two last dimensions rotated by 180 degrees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's find the gradient with respect to the kernel:\n",
    "\n",
    "$$\\nabla L_{\\text{kernel}}(x)_{o_ki_kh_kw_k} =\n",
    "\\sum_{\n",
    "b \\in 0\\dots B-1, \\, \n",
    "o \\in 0\\dots O-1,\\,\n",
    "h \\in 0\\dots H+2P -K,\\,\n",
    "w \\in 0\\dots W+2P -K\n",
    "}\n",
    "\\cfrac{\\partial f}{\\partial \\text{kernel}_{o_ki_kh_kw_k}} (x)_{bohw}\n",
    "\\nabla \\hat L_{bohw} = \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$=\n",
    "\\sum_{b,\\,o,\\,h,\\,w}\\,\n",
    "\\sum_{i',\\,h_k',\\,w_k'}\n",
    "\\text{padded}(x, p)_{b,i',h + h_k', w+w_k'}\n",
    "\\cfrac{\\partial\\text{kernel}_{o i' h_k' w_k'}}{\\partial \\text{kernel}_{o_ki_kh_kw_k}} \\cdot \n",
    "\\nabla \\hat L_{bohw} =\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$=\n",
    "\\sum_{b,\\,o,\\,h,\\,w}\\,\n",
    "\\sum_{i',\\,h_k',\\,w_k'}\n",
    "\\text{padded}(x, p)_{b,i',h + h_k', w+w_k'}\n",
    "[o == o_k \\land i'==i_k \\land h_k' == h_k \\land w'_k== w_k]\n",
    "\\nabla \\hat L_{bohw}=\\\\=\n",
    "\\sum_{b,\\,h,\\,w}\n",
    "\\text{padded}(x, p)_{b,i_k,h + h_k, w+w_k}\n",
    "\\nabla \\hat L_{bohw}=\n",
    "\\text{conv2d}^T(x^T;\\nabla \\hat L^T,p)_{o_ki_kh_kw_k}$$\n",
    "\n",
    "Where $\\text{conv2d}^T, x^T$ and $\\nabla \\hat L^T$ have their first two dimensions transposed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the gradient with respect to the bias:\n",
    "\n",
    "$$\\nabla L_{\\text{bias}}(x)_{o} =\n",
    "\\sum_{\n",
    "b \\in 0\\dots B-1, \\, \n",
    "o' \\in 0\\dots O-1,\\,\n",
    "h \\in 0\\dots H+2P -K,\\,\n",
    "w \\in 0\\dots W+2P -K\n",
    "}\n",
    "\\cfrac{\\partial \\text{bias}_{o'}}{\\partial \\text{bias}_{o}} (x)_{bohw}\n",
    "\\nabla \\hat L_{bohw} = \n",
    "\\sum_{\n",
    "bhw\n",
    "}\n",
    "\\nabla \\hat L_{bohw} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import scipy.signal\n",
    "import skimage\n",
    "\n",
    "\n",
    "class Conv2d(Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size):\n",
    "        super(Conv2d, self).__init__()\n",
    "        assert kernel_size % 2 == 1, kernel_size\n",
    "\n",
    "        stdv = 1.0 / np.sqrt(in_channels)\n",
    "        self.W = np.random.uniform(\n",
    "            -stdv, stdv, size=(out_channels, in_channels, kernel_size, kernel_size)\n",
    "        )\n",
    "        self.b = np.random.uniform(-stdv, stdv, size=(out_channels,))\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        self.gradW = np.zeros_like(self.W)\n",
    "        self.gradb = np.zeros_like(self.b)\n",
    "\n",
    "    @staticmethod\n",
    "    def _convolve2d(input, weight, padding=0):\n",
    "        minibatch, in_channels, iH, iW = input.shape\n",
    "        out_channels, in_channels_weight, kH, kW = weight.shape\n",
    "\n",
    "        assert in_channels == in_channels_weight\n",
    "\n",
    "        output = np.zeros(\n",
    "            (\n",
    "                minibatch,\n",
    "                out_channels,\n",
    "                iH + 2 * padding + 1 - kH,\n",
    "                iW + 2 * padding + 1 - kW,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        input = np.pad(input, ((0, 0), (0, 0), (padding, padding), (padding, padding)))\n",
    "\n",
    "        for out in range(out_channels):\n",
    "            output[:, out] = sp.signal.correlate(\n",
    "                input, weight[out, None], mode=\"valid\"\n",
    "            ).squeeze(1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def _compute_output(self, input):\n",
    "        self._output = self._convolve2d(input, self.W, padding=self.kernel_size // 2)\n",
    "\n",
    "        self._output += np.expand_dims(self.b, (0, 2, 3))\n",
    "\n",
    "        return self._output\n",
    "\n",
    "    def _compute_input_grad(self, input, gradOutput):\n",
    "        self._input_grad = self._convolve2d(\n",
    "            gradOutput,\n",
    "            np.rot90(self.W.transpose(1, 0, 2, 3), k=2, axes=(2, 3)),\n",
    "            padding=self.kernel_size // 2,\n",
    "        )\n",
    "\n",
    "        return self._input_grad\n",
    "\n",
    "    def accGradParameters(self, input, gradOutput):\n",
    "        self.gradW = self._convolve2d(\n",
    "            input.transpose(1, 0, 2, 3),\n",
    "            gradOutput.transpose(1, 0, 2, 3),\n",
    "            padding=self.kernel_size // 2,\n",
    "        ).transpose(1, 0, 2, 3)\n",
    "\n",
    "        self.gradb = gradOutput.sum(axis=(0, 2, 3))\n",
    "\n",
    "    def zeroGradParameters(self):\n",
    "        self.gradW.fill(0)\n",
    "        self.gradb.fill(0)\n",
    "\n",
    "    def getParameters(self):\n",
    "        return [self.W, self.b]\n",
    "\n",
    "    def getGradParameters(self):\n",
    "        return [self.gradW, self.gradb]\n",
    "\n",
    "    def __repr__(self):\n",
    "        s = self.W.shape\n",
    "        q = \"Conv2d %d -> %d\" % (s[1], s[0])\n",
    "        return q"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "1.modules.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "f98f21f0b58c314391d9edda6a890b43799e7bbdcfa23cfcf4ab03be958beb23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
