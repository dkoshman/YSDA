{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b6e7916",
   "metadata": {},
   "source": [
    "### Кошман Дмитрий\n",
    "\n",
    "$\\newcommand{\\Sum}{\\sum\\limits}\n",
    "\\newcommand{\\Int}{\\int\\limits}\n",
    "\\newcommand{\\Intf}{\\int\\limits_{-\\infty}^{+\\infty}}\n",
    "\\newcommand{\\Prod}{\\prod\\limits}\n",
    "\\newcommand{\\Max}{\\max\\limits}\n",
    "\\newcommand{\\Min}{\\min\\limits}\n",
    "\\newcommand{\\Lim}{\\lim\\limits}\n",
    "\\newcommand{\\Var}{\\mathbb{V}}\n",
    "\\newcommand{\\Exp}{\\mathbb{E}}\n",
    "\\newcommand{\\RR}{\\mathbb{R}}\n",
    "\\newcommand{\\argmax}{\\arg\\max}\n",
    "\\newcommand{\\Cov}{\\text{Cov}}\n",
    "\\newcommand{\\makebold}[1]{\\boldsymbol{#1}}\n",
    "\\newcommand{\\eps}{\\varepsilon}\n",
    "\\newcommand{\\mean}[1]{\\overline{#1}}\n",
    "\\newcommand{\\avg}[1]{\\langle #1 \\rangle}\n",
    "\\newcommand{\\angmean}[1]{\\langle #1 \\rangle}\n",
    "\\newcommand{\\Prob}{\\mathcal{P}}\n",
    "\\newcommand{\\se}{\\text{se}}\n",
    "\\newcommand{\\lp}{\\left}\n",
    "\\newcommand{\\rp}{\\right}\n",
    "\\newcommand{\\boldx}{\\boldsymbol{x}}\n",
    "\\newcommand{\\boldy}{\\boldsymbol{y}}\n",
    "\\newcommand{\\boldz}{\\boldsymbol{z}}\n",
    "\\newcommand{\\boldX}{\\boldsymbol{X}}\n",
    "\\newcommand{\\boldY}{\\boldsymbol{Y}}\n",
    "\\newcommand{\\boldZ}{\\boldsymbol{Z}}\n",
    "\\newcommand{\\Poisson}{\\mathrm{Poisson}}\n",
    "\\newcommand{\\Triangle}{\\mathrm{Triangle}}\n",
    "\\newcommand{\\Uniform}{\\mathrm{Uniform}}\n",
    "\\newcommand{\\Binomial}{\\mathrm{Binomial}}\n",
    "\\newcommand{\\Bernoulli}{\\mathrm{Bernoulli}}\n",
    "\\newcommand{\\Gammap}{\\mathrm{Gamma}}\n",
    "\\newcommand{\\Normal}{\\mathcal{N}}\n",
    "\\newcommand{\\LogN}{\\mathrm{LogN}}\n",
    "\\newcommand{\\Exponential}{\\mathrm{Exp}}\n",
    "\\newcommand{\\Erlang}{\\mathrm{Erlang}}\n",
    "\\newcommand{\\Cauchy}{C}\n",
    "\\newcommand{\\Dir}{\\mathrm{Dir}}\n",
    "\\newcommand{\\Beta}{\\mathrm{Beta}}\n",
    "\\newcommand{\\Pareto}{\\mathrm{Pareto}}\n",
    "\\newcommand{\\lf}{\\left\\{}\n",
    "\\newcommand{\\rf}{\\right\\}}\n",
    "\\newcommand{\\lp}{\\left(}\n",
    "\\newcommand{\\rp}{\\right)}\n",
    "\\newcommand{\\Ecdf}[1]{\\hat{F}_n(#1)}\n",
    "\\newcommand{\\OPT}{\\text{OPT}}\n",
    "\\newcommand{\\opt}{\\text{opt}}\n",
    "\\newcommand{\\boot}{\\text{boot}}\n",
    "\\newcommand{\\bias}{\\text{bias}}\n",
    "\\newcommand{\\se}{\\text{se}}\n",
    "\\newcommand{\\MSE}{\\text{MSE}}\n",
    "\\newcommand{\\qm}{\\text{qm}}\n",
    "\\newcommand{\\as}{\\text{as}}\n",
    "\\newcommand{\\trace}{\\text{trace}}\n",
    "\\newcommand{\\esttheta}{\\hat{\\theta}}\n",
    "\\newcommand{\\estlambda}{\\hat{\\lambda}}\n",
    "\\newcommand{\\estmu}{\\hat{\\mu}}\n",
    "\\newcommand{\\estsigma}{\\hat{\\sigma}}\n",
    "\\newcommand{\\estalpha}{\\hat{\\alpha}}\n",
    "\\newcommand{\\estbeta}{\\hat{\\beta}}\n",
    "\\newcommand{\\estxi}{\\hat{\\xi}}\n",
    "\\newcommand{\\esttau}{\\hat{\\tau}}\n",
    "\\newcommand{\\estpsi}{\\hat{\\psi}}\n",
    "\\newcommand{\\esta}{\\hat{a}}\n",
    "\\newcommand{\\estb}{\\hat{b}}\n",
    "\\newcommand{\\estc}{\\hat{c}}\n",
    "\\newcommand{\\estd}{\\hat{d}}\n",
    "\\newcommand{\\estp}{\\hat{p}}\n",
    "\\newcommand{\\estT}{\\hat{T}}\n",
    "\\newcommand{\\estR}{\\hat{R}}\n",
    "\\newcommand{\\estF}{\\hat{F}}\n",
    "\\newcommand{\\estf}{\\hat{f}}\n",
    "\\newcommand{\\estC}{\\hat{C}}\n",
    "\\newcommand{\\estS}{\\hat{S}}\n",
    "\\newcommand{\\estY}{\\hat{Y}}\n",
    "\\newcommand{\\esty}{\\hat{y}}\n",
    "\\newcommand{\\estVar}{\\hat{\\Var}}\n",
    "\\newcommand{\\estExp}{\\hat{\\Exp}}\n",
    "\\newcommand{\\estSe}{\\hat{\\se}}\n",
    "\\newcommand{\\hatp}{\\hat{p}}\n",
    "\\newcommand{\\hatF}{\\hat{F}}\n",
    "\\newcommand{\\hatT}{\\hat{T}}\n",
    "\\newcommand{\\hattheta}{\\hat{\\theta}}\n",
    "\\newcommand{\\hatse}{\\hat{\\se}}\n",
    "\\\n",
    "\\newcommand{\\MLE}{\\text{MLE}}\n",
    "\\newcommand{\\mlexi}{\\xi_{MLE}}\n",
    "\\newcommand{\\mletheta}{\\theta_{MLE}}\n",
    "\\newcommand{\\mlelambda}{\\lambda_{MLE}}\n",
    "\\newcommand{\\mlesigma}{\\sigma_{MLE}}\n",
    "\\newcommand{\\mlepsi}{\\psi_{MLE}}\n",
    "\\newcommand{\\mlemu}{\\mu_{MLE}}\n",
    "\\newcommand{\\mlenu}{\\nu_{MLE}}\n",
    "\\\n",
    "\\newcommand{\\tilx}{\\tilde{x}}\n",
    "\\newcommand{\\tily}{\\tilde{y}}\n",
    "\\newcommand{\\tilX}{\\tilde{X}}\n",
    "\\newcommand{\\tilY}{\\tilde{Y}}\n",
    "\\newcommand{\\tilK}{\\tilde{K}}\n",
    "\\newcommand{\\tilU}{\\tilde{U}}\n",
    "\\newcommand{\\tilV}{\\tilde{V}}\n",
    "\\newcommand{\\tilSigma}{\\tilde{\\Sigma}}\n",
    "\\newcommand{\\tiltau}{\\tilde{\\tau}}\n",
    "\\newcommand{\\tiltheta}{\\tilde{\\theta}}\n",
    "\\newcommand{\\tillambda}{\\tilde{\\lambda}}\n",
    "\\newcommand{\\tilsigma}{\\tilde{\\sigma}}\n",
    "\\newcommand{\\tilpsi}{\\tilde{\\psi}}\n",
    "\\newcommand{\\tilmu}{\\tilde{\\mu}}\n",
    "\\\n",
    "\\newcommand{\\esttheta}{\\hat{\\theta}}\n",
    "\\newcommand{\\estlambda}{\\hat{\\lambda}}\n",
    "\\newcommand{\\estmu}{\\hat{\\mu}}\n",
    "\\newcommand{\\estsigma}{\\hat{\\sigma}}\n",
    "\\newcommand{\\estalpha}{\\hat{\\alpha}}\n",
    "\\newcommand{\\estbeta}{\\hat{\\beta}}\n",
    "\\newcommand{\\estxi}{\\hat{\\xi}}\n",
    "\\newcommand{\\esttau}{\\hat{\\tau}}\n",
    "\\newcommand{\\estpsi}{\\hat{\\psi}}\n",
    "\\newcommand{\\esta}{\\hat{a}}\n",
    "\\newcommand{\\estb}{\\hat{b}}\n",
    "\\newcommand{\\estc}{\\hat{c}}\n",
    "\\newcommand{\\estd}{\\hat{d}}\n",
    "\\newcommand{\\estf}{\\hat{f}}\n",
    "\\newcommand{\\estp}{\\hat{p}}\n",
    "\\newcommand{\\esty}{\\hat{y}}\n",
    "\\newcommand{\\estT}{\\hat{T}}\n",
    "\\newcommand{\\estR}{\\hat{R}}\n",
    "\\newcommand{\\estF}{\\hat{F}}\n",
    "\\newcommand{\\estC}{\\hat{C}}\n",
    "\\newcommand{\\estS}{\\hat{S}}\n",
    "\\newcommand{\\estY}{\\hat{Y}}\n",
    "\\newcommand{\\estVar}{\\hat{\\Var}}\n",
    "\\newcommand{\\estExp}{\\hat{\\Exp}}\n",
    "\\newcommand{\\estSe}{\\hat{\\se}}\n",
    "\\\n",
    "\\newcommand{\\ecdf}{\\hat{F}}\n",
    "\\\n",
    "\\newcommand{\\hata}{\\hat{a}}\n",
    "\\newcommand{\\hatb}{\\hat{b}}\n",
    "\\newcommand{\\hatc}{\\hat{c}}\n",
    "\\newcommand{\\hatd}{\\hat{d}}\n",
    "\\newcommand{\\hatf}{\\hat{f}}\n",
    "\\newcommand{\\hatg}{\\hat{g}}\n",
    "\\newcommand{\\hatk}{\\hat{k}}\n",
    "\\newcommand{\\hatp}{\\hat{p}}\n",
    "\\newcommand{\\hatr}{\\hat{r}}\n",
    "\\newcommand{\\hatt}{\\hat{t}}\n",
    "\\newcommand{\\haty}{\\hat{y}}\n",
    "\\newcommand{\\hatw}{\\hat{w}}\n",
    "\\\n",
    "\\newcommand{\\hatC}{\\hat{C}}\n",
    "\\newcommand{\\hatF}{\\hat{F}}\n",
    "\\newcommand{\\hatJ}{\\hat{J}}\n",
    "\\newcommand{\\hatK}{\\hat{K}}\n",
    "\\newcommand{\\hatP}{\\hat{P}}\n",
    "\\newcommand{\\hatS}{\\hat{S}}\n",
    "\\newcommand{\\hatT}{\\hat{T}}\n",
    "\\newcommand{\\hatY}{\\hat{Y}}\n",
    "\\newcommand{\\hatV}{\\hat{V}}\n",
    "\\newcommand{\\hatU}{\\hat{U}}\n",
    "\\\n",
    "\\newcommand{\\hateps}{\\hat{\\eps}}\n",
    "\\newcommand{\\hatalpha}{\\hat{\\alpha}}\n",
    "\\newcommand{\\hatbeta}{\\hat{\\beta}}\n",
    "\\newcommand{\\hatpsi}{\\hat{\\psi}}\n",
    "\\newcommand{\\hatlambda}{\\hat{\\lambda}}\n",
    "\\newcommand{\\hattheta}{\\hat{\\theta}}\n",
    "\\newcommand{\\hatsigma}{\\hat{\\sigma}}\n",
    "\\newcommand{\\hatmu}{\\hat{\\mu}}\n",
    "\\newcommand{\\hatnu}{\\hat{\\nu}}\n",
    "\\newcommand{\\hatSigma}{\\hat{\\Sigma}}\n",
    "\\newcommand{\\hatSe}{\\hat{\\se}}\n",
    "\\newcommand{\\hatExp}{\\hat{\\Exp}}\n",
    "\\newcommand{\\hatVar}{\\hat{\\Var}}\n",
    "\\\n",
    "\\newcommand{\\RejectRegion}{R}\n",
    "\\newcommand{\\pvalue}{\\text{p-value}}\n",
    "\\newcommand{\\llr}{\\ell}\n",
    "\\newcommand{\\Llr}{\\mathcal{L}}\n",
    "\\\n",
    "\\newcommand{\\Distr}{\\mathsf{D}}\n",
    "\\newcommand{\\bolda}{\\boldsymbol{a}}\n",
    "\\newcommand{\\boldb}{\\boldsymbol{b}}\n",
    "\\newcommand{\\boldc}{\\boldsymbol{c}}\n",
    "\\newcommand{\\boldd}{\\boldsymbol{d}}\n",
    "\\newcommand{\\bolde}{\\boldsymbol{e}}\n",
    "\\newcommand{\\boldf}{\\boldsymbol{f}}\n",
    "\\newcommand{\\boldg}{\\boldsymbol{g}}\n",
    "\\newcommand{\\boldh}{\\boldsymbol{h}}\n",
    "\\newcommand{\\boldi}{\\boldsymbol{i}}\n",
    "\\newcommand{\\boldj}{\\boldsymbol{j}}\n",
    "\\newcommand{\\boldk}{\\boldsymbol{k}}\n",
    "\\newcommand{\\boldl}{\\boldsymbol{l}}\n",
    "\\newcommand{\\boldm}{\\boldsymbol{m}}\n",
    "\\newcommand{\\boldn}{\\boldsymbol{n}}\n",
    "\\newcommand{\\boldo}{\\boldsymbol{o}}\n",
    "\\newcommand{\\boldp}{\\boldsymbol{p}}\n",
    "\\newcommand{\\boldq}{\\boldsymbol{q}}\n",
    "\\newcommand{\\boldr}{\\boldsymbol{r}}\n",
    "\\newcommand{\\bolds}{\\boldsymbol{s}}\n",
    "\\newcommand{\\boldt}{\\boldsymbol{t}}\n",
    "\\newcommand{\\boldu}{\\boldsymbol{u}}\n",
    "\\newcommand{\\boldv}{\\boldsymbol{v}}\n",
    "\\newcommand{\\boldw}{\\boldsymbol{w}}\n",
    "\\newcommand{\\boldx}{\\boldsymbol{x}}\n",
    "\\newcommand{\\boldy}{\\boldsymbol{y}}\n",
    "\\newcommand{\\boldz}{\\boldsymbol{z}}\n",
    "\\\n",
    "\\newcommand{\\hatboldx}{\\hat{\\boldx}}\n",
    "\\newcommand{\\hatboldk}{\\hat{\\boldk}}\n",
    "\\newcommand{\\hatboldw}{\\hat{\\boldw}}\n",
    "\\newcommand{\\hatboldp}{\\hat{\\boldp}}\n",
    "\\newcommand{\\hatboldK}{\\hat{\\boldK}}\n",
    "\\newcommand{\\hatboldC}{\\hat{\\boldC}}\n",
    "\\newcommand{\\hatboldS}{\\hat{\\boldS}}\n",
    "\\newcommand{\\hatboldU}{\\hat{\\boldU}}\n",
    "\\newcommand{\\hatboldV}{\\hat{\\boldV}}\n",
    "\\newcommand{\\hatboldX}{\\hat{\\boldX}}\n",
    "\\newcommand{\\hatboldSigma}{\\hat{\\boldSigma}}\n",
    "\\newcommand{\\hatboldLambda}{\\hat{\\boldLambda}}\n",
    "\\newcommand{\\hatboldy}{\\hat{\\boldy}}\n",
    "\\newcommand{\\hatboldmu}{\\hat{\\boldmu}}\n",
    "\\newcommand{\\hatboldalpha}{\\hat{\\boldalpha}}\n",
    "\\newcommand{\\hatboldbeta}{\\hat{\\boldbeta}}\n",
    "\\newcommand{\\hatboldgamma}{\\hat{\\boldgamma}}\n",
    "\\newcommand{\\hatboldtheta}{\\hat{\\bold\\theta}}\n",
    "\\newcommand{\\hatboldeps}{\\hat{\\boldeps}}\n",
    "\\newcommand{\\hatbolddelta}{\\hat{\\bolddelta}}\n",
    "\\\n",
    "\\newcommand{\\lp}{\\left(}\n",
    "\\newcommand{\\rp}{\\right)}\n",
    "\\newcommand{\\lf}{\\left\\{}\n",
    "\\newcommand{\\rf}{\\right\\}}\n",
    "\\newcommand{\\ls}{\\left[}\n",
    "\\newcommand{\\rs}{\\right]}\n",
    "\\newcommand{\\lv}{\\left|}\n",
    "\\newcommand{\\rv}{\\right|}\n",
    "\\newcommand{\\la}{\\left\\langle}\n",
    "\\newcommand{\\ra}{\\right\\rangle}\n",
    "$\n",
    "\n",
    "$\n",
    "\\newcommand{\\boldA}{\\boldsymbol{A}}\n",
    "\\newcommand{\\boldB}{\\boldsymbol{B}}\n",
    "\\newcommand{\\boldC}{\\boldsymbol{C}}\n",
    "\\newcommand{\\boldD}{\\boldsymbol{D}}\n",
    "\\newcommand{\\boldE}{\\boldsymbol{E}}\n",
    "\\newcommand{\\boldF}{\\boldsymbol{F}}\n",
    "\\newcommand{\\boldH}{\\boldsymbol{H}}\n",
    "\\newcommand{\\boldJ}{\\boldsymbol{J}}\n",
    "\\newcommand{\\boldK}{\\boldsymbol{K}}\n",
    "\\newcommand{\\boldL}{\\boldsymbol{L}}\n",
    "\\newcommand{\\boldM}{\\boldsymbol{M}}\n",
    "\\newcommand{\\boldN}{\\boldsymbol{N}}\n",
    "\\newcommand{\\boldI}{\\boldsymbol{I}}\n",
    "\\newcommand{\\boldP}{\\boldsymbol{P}}\n",
    "\\newcommand{\\boldQ}{\\boldsymbol{Q}}\n",
    "\\newcommand{\\boldR}{\\boldsymbol{R}}\n",
    "\\newcommand{\\boldS}{\\boldsymbol{S}}\n",
    "\\newcommand{\\boldT}{\\boldsymbol{T}}\n",
    "\\newcommand{\\boldO}{\\boldsymbol{O}}\n",
    "\\newcommand{\\boldU}{\\boldsymbol{U}}\n",
    "\\newcommand{\\boldV}{\\boldsymbol{V}}\n",
    "\\newcommand{\\boldW}{\\boldsymbol{W}}\n",
    "\\newcommand{\\boldX}{\\boldsymbol{X}}\n",
    "\\newcommand{\\boldY}{\\boldsymbol{Y}}\n",
    "\\newcommand{\\boldZ}{\\boldsymbol{Z}}\n",
    "\\newcommand{\\boldXY}{\\boldsymbol{XY}}\n",
    "\\\n",
    "\\newcommand{\\Family}{\\mathfrak{F}}\n",
    "\\\n",
    "\\newcommand{\\argmax}{\\arg\\max}\n",
    "\\newcommand{\\argmin}{\\arg\\min}\n",
    "\\\n",
    "\\newcommand{\\boldalpha}{\\boldsymbol{\\alpha}}\n",
    "\\newcommand{\\boldbeta}{\\boldsymbol{\\beta}}\n",
    "\\newcommand{\\boldtheta}{\\boldsymbol{\\theta}}\n",
    "\\newcommand{\\boldmu}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\boldxi}{\\boldsymbol{\\xi}}\n",
    "\\newcommand{\\boldeta}{\\boldsymbol{\\eta}}\n",
    "\\newcommand{\\boldpi}{\\boldsymbol{\\pi}}\n",
    "\\newcommand{\\boldsigma}{\\boldsymbol{\\sigma}}\n",
    "\\newcommand{\\boldphi}{\\boldsymbol{\\phi}}\n",
    "\\newcommand{\\boldpsi}{\\boldsymbol{\\psi}}\n",
    "\\newcommand{\\boldlambda}{\\boldsymbol{\\lambda}}\n",
    "\\newcommand{\\boldgamma}{\\boldsymbol{\\gamma}}\n",
    "\\newcommand{\\bolddelta}{\\boldsymbol{\\delta}}\n",
    "\\newcommand{\\boldeps}{\\boldsymbol{\\eps}}\n",
    "\\newcommand{\\boldPhi}{\\boldsymbol{\\Phi}}\n",
    "\\newcommand{\\boldPsi}{\\boldsymbol{\\Psi}}\n",
    "\\newcommand{\\boldLambda}{\\boldsymbol{\\Lambda}}\n",
    "\\newcommand{\\boldSigma}{\\boldsymbol{\\Sigma}}\n",
    "\\newcommand{\\boldTheta}{\\boldsymbol{\\Theta}}\n",
    "\\newcommand{\\boldOmega}{\\boldsymbol{\\Omega}}\n",
    "\\\n",
    "\\newcommand{\\boldzero}{\\boldsymbol{0}}\n",
    "\\newcommand{\\boldones}{\\boldsymbol{1}}\n",
    "\\newcommand{\\boldone}{\\boldsymbol{1}}\n",
    "\\\n",
    "\\newcommand{\\CC}{\\mathbb{C}}\n",
    "\\newcommand{\\NN}{\\mathbb{N}}\n",
    "\\newcommand{\\PP}{\\mathbb{P}}\n",
    "\\newcommand{\\RR}{\\mathbb{R}}\n",
    "\\newcommand{\\XX}{\\mathbb{X}}\n",
    "\\newcommand{\\ZZ}{\\mathbb{Z}}\n",
    "\\renewcommand{\\AA}{\\mathbb{A}}\n",
    "\\\n",
    "\\newcommand{\\mcalA}{\\mathcal{A}}\n",
    "\\newcommand{\\mcalB}{\\mathcal{B}}\n",
    "\\newcommand{\\mcalC}{\\mathcal{C}}\n",
    "\\newcommand{\\mcalD}{\\mathcal{D}}\n",
    "\\newcommand{\\mcalE}{\\mathcal{E}}\n",
    "\\newcommand{\\mcalF}{\\mathcal{F}}\n",
    "\\newcommand{\\mcalI}{\\mathcal{I}}\n",
    "\\newcommand{\\mcalL}{\\mathcal{L}}\n",
    "\\newcommand{\\mcalP}{\\mathcal{P}}\n",
    "\\newcommand{\\mcalQ}{\\mathcal{Q}}\n",
    "\\newcommand{\\mcalX}{\\mathcal{X}}\n",
    "\\newcommand{\\hatmcalB}{\\hat{\\mcalB}}\n",
    "\\\n",
    "\\newcommand{\\Ecdf}[1]{\\hat{F}_n(#1)}\n",
    "\\newcommand{\\OPT}{\\ensuremath{\\mathrm{OPT}\\xspace}}\n",
    "\\newcommand{\\opt}{\\ensuremath{\\mathrm{opt}\\xspace}}\n",
    "\\newcommand{\\boot}{\\ensuremath{\\mathrm{boot}\\xspace}}\n",
    "\\newcommand{\\bias}{\\ensuremath{\\mathrm{bias}\\xspace}}\n",
    "\\newcommand{\\se}{\\ensuremath{\\mathrm{se}\\xspace}}\n",
    "\\newcommand{\\MSE}{\\ensuremath{\\mathrm{MSE}\\xspace}}\n",
    "\\newcommand{\\RSS}{\\ensuremath{\\mathrm{RSS}\\xspace}}\n",
    "\\newcommand{\\qm}{\\ensuremath{\\mathrm{qm}\\xspace}}\n",
    "\\newcommand{\\as}{\\ensuremath{\\mathrm{as}\\xspace}}\n",
    "\\newcommand{\\trace}{\\ensuremath{\\mathrm{tr}\\xspace}}\n",
    "\\newcommand{\\const}{\\ensuremath{\\mathrm{const}\\xspace}}\n",
    "\\newcommand{\\sign}{\\ensuremath{\\mathrm{sign}\\xspace}}\n",
    "\\newcommand{\\tr}{\\mathrm{tr}}\n",
    "\\newcommand{\\new}{\\mathrm{new}}\n",
    "\\newcommand{\\lasso}{\\mathrm{lasso}}\n",
    "\\newcommand{\\old}{\\mathrm{old}}\n",
    "\\newcommand{\\diag}{\\mathrm{diag}}\n",
    "\\newcommand{\\rank}{\\mathrm{rg}}\n",
    "\\newcommand{\\ML}{\\mathrm{ML}}\n",
    "\\newcommand{\\MP}{\\mathrm{MP}}\n",
    "\\newcommand{\\KL}{\\mathrm{KL}}\n",
    "\\newcommand{\\NV}{\\mathrm{NV}}\n",
    "\\newcommand{\\MV}{\\mathrm{MV}}\n",
    "\\newcommand{\\NP}{\\mathrm{MP}}   % Нейман-Пирсон\n",
    "\\newcommand{\\vs}{\\mathrm{vs}}   % versus\n",
    "\\newcommand{\\LOO}{\\mathrm{LOO}}\n",
    "\\newcommand{\\IGMV}{\\mathrm{IGMV}}\n",
    "\\newcommand{\\MM}{\\mathrm{MM}}\n",
    "\\newcommand{\\nat}{\\mathrm{nat}\\xspace}\n",
    "\\newcommand{\\grad}{\\mathrm{grad}\\xspace}\n",
    "\\\n",
    "\\newcommand{\\Covariance}{\\Sigma}\n",
    "\\newcommand{\\CovX}{\\Covariance_{\\boldX}}\n",
    "\\newcommand{\\CovY}{\\Covariance_{\\boldY}}\n",
    "\\newcommand{\\CovZ}{\\Covariance_{\\boldZ}}\n",
    "\\newcommand{\\CovXY}{\\Covariance_{\\boldX\\boldY}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe858b08",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- https://github.com/siri3us/ASML/blob/master/notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767ee00b",
   "metadata": {},
   "source": [
    "## Теоретический блок"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc4a03e",
   "metadata": {},
   "source": [
    "### [Задача 1 [3 балла]](#Задача-1) \n",
    "### [Задача 2 [4 балла]](#Задача-2) \n",
    "### [Задача 3 [5 баллов]](#Задача-3)\n",
    "### [Задача 4 [3 балла]](#Задача-4) \n",
    "### [Задача 5 [5 баллов]](#Задача-5)\n",
    "### [Задача 6 [20 баллов]](#Задача-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f990dbe3",
   "metadata": {},
   "source": [
    "## Теоретический блок"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2625874c",
   "metadata": {},
   "source": [
    "### Задача 1  \n",
    "Пусть дана выборка $(\\boldX, \\boldt)$, $\\boldX \\in \\RR^{N\\times D}$, $\\boldt \\in \\RR^N$, где $\\boldX$ –– матрица объектов-признаков, и $\\boldt = (t_1,\\ldots,t_N)^T$ –– набор целевых значений. Предположим, что справедлива следующая модель данных: $$ t = \\boldw^T\\boldphi(\\boldx) + \\eps, $$ где $\\boldx \\in \\RR^D$, $\\boldphi(\\cdot)$ –– некоторая вектор-функция, которая задает новое признаковое описание объекта $\\boldx$ в пространстве размерности $M$,  $\\eps\\sim \\Normal(\\eps|0,\\beta^{-1})$.  Для краткости вместо $\\boldphi(\\boldx_i)$ будем писать просто $\\boldphi_i$. Обозначим через $\\Phi_N$ матрицу объектов признаков в новом признаковом пространстве: $$ \\boldphi(\\boldx) = \\begin{pmatrix} \t\\phi_1(\\boldx)\\\\ \t\\vdots\\\\ \t\\phi_M(\\boldx) \\end{pmatrix}, \\qquad  \\Phi_N \\triangleq \\begin{pmatrix} \t\\boldphi_1^T\\\\ \t\\vdots\\\\ \t\\boldphi_N^T \\end{pmatrix} = \\begin{pmatrix} \t\\boldphi(\\boldx_1)^T\\\\ \t\\vdots\\\\ \t\\boldphi(\\boldx_N)^T \\end{pmatrix} = \\begin{pmatrix} \t\\phi_1(\\boldx_1) &\\dots &\\phi_M(\\boldx_1)\\\\ \t\\vdots &\\ddots &\\vdots\\\\ \t\\phi_1(\\boldx_N) &\\dots &\\phi_M(\\boldx_N)\\\\ \\end{pmatrix} \\in \\RR^{N \\times M}. $$  Пусть имеет место следующее априорное распределение на вектор весов $\\boldw$: $p(\\boldw|\\alpha) = N(\\boldw|{\\bf 0},\\alpha^{-1}\\boldI)$. В таком случае легко подсчитать, что плотность апостериорного распределение равна $p(\\boldw|\\boldt,\\alpha) = N(\\boldw|\\boldm_N,\\boldS_N)$, где  $$ \\boldm_N = \\beta\\boldS_N\\Phi^T\\boldt,\\qquad \\boldS_N^{-1} = \\alpha\\boldI + \\beta\\Phi^T\\Phi. $$ Апостериорное распределение предсказания $t$ в некоторой точке $\\boldx$ равно $$ p(t|\\boldx,\\boldt,\\alpha,\\beta) = \\int p(t|\\boldw,\\beta)p(\\boldw|\\boldt,\\alpha,\\beta)d\\boldw = N(t|\\boldm_N^T\\boldphi(\\boldx),\\sigma_N^2(\\boldx)), $$ где $$ \\sigma_N^2(\\boldx) = \\frac{1}{\\beta} + \\boldphi(\\boldx)^T\\boldS_N\\boldphi(\\boldx). $$ Используя равенство $$ (\\boldM + \\boldv\\boldv^T)^{-1} = \\boldM^{-1} - \\frac{(\\boldM^{-1}\\boldv)(\\boldv^T\\boldM^{-1})}{1+\\boldv^T\\boldM^{-1}\\boldv}, $$ покажите, что $$ \\sigma_{N+1}^2(\\boldx)\\leq\\sigma_N^2(\\boldx). $$  \n",
    "\n",
    "---\n",
    "### Решение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061cf83c",
   "metadata": {},
   "source": [
    "Посмотрим, как зависит $\\Phi_{N+1}^T\\Phi_{N+1}$ от $\\Phi_N^T\\Phi_N$. Если считать, что при увеличении $N$ к нам инкрементально поступают новые наблюдения $(\\boldx_{N+1}, \\boldphi_{N+1})$, а не новые независимые выборки размера $N+1$, то $(\\Phi_{N+1})_{i} = (\\Phi_{N})_{i},\\, i = \\{1,\\dots,N\\}$, и \n",
    "$$\n",
    "(\\Phi_{N+1}^T\\Phi_{N+1})_{ij} = \n",
    "\\sum_{k=1}^{N+1} (\\Phi_{N+1}^T)_{ik} (\\Phi_{N+1})_{kj} =\n",
    "\\sum_{k=1}^{N+1} (\\Phi_{N+1})_{ki} (\\Phi_{N+1})_{kj} =\\\\=\n",
    "\\sum_{k=1}^{N} (\\Phi_{N+1})_{ki} (\\Phi_{N+1})_{kj} + (\\Phi_{N+1})_{ N+1,i} (\\Phi_{N+1})_{N+1,j}=\n",
    "\\sum_{k=1}^{N} (\\Phi_{N})_{ki} (\\Phi_{N})_{kj} + (\\boldphi_{N+1})_i (\\boldphi_{N+1})_j=\\\\=\n",
    "(\\Phi_{N}^T\\Phi_{N})_{ij} + (\\boldphi_{N+1})_i (\\boldphi_{N+1})_j\n",
    "$$\n",
    "\n",
    "То есть $\\Phi_{N+1}^T\\Phi_{N+1} = \\Phi_N^T\\Phi_N + \\boldphi_{N+1}\\boldphi_{N+1}^T$, и $S_{N+1}$ можно представить в следующем виде:\n",
    "\n",
    "$$\n",
    "\\boldS_{N+1} = \n",
    "(\\alpha\\boldI + \\beta\\Phi_{N+1}^T\\Phi_{N+1})^{-1} =\n",
    "(\\alpha\\boldI + \\beta\\Phi_N^T\\Phi_N + \\beta\\boldphi_{N+1}\\boldphi_{N+1}^T)^{-1}\n",
    "$$\n",
    "\n",
    "Теперь используем приведенное в условие равенство, где $\\boldM = \\alpha\\boldI + \\beta\\Phi_N^T\\Phi_N, \\, \\boldv = \\sqrt\\beta\\boldphi_{N+1}$, сперва заметив, что $M^{-1} = \\boldS_N$:\n",
    "\n",
    "$$\n",
    "\\boldS_{N+1} = \n",
    "\\boldS_N - \\frac{\\boldS_N\\boldv\\boldv^T\\boldS_N}{1+\\boldv^T\\boldS_N\\boldv}\n",
    "$$\n",
    "\n",
    "Посмотрим на интересующую нас разницу:\n",
    "\n",
    "$$ \\sigma_N^2(\\boldx) - \\sigma_{N+1}^2(\\boldx) = \n",
    "\\frac{1}{\\beta} + \\boldphi(\\boldx)^T\\boldS_N\\boldphi(\\boldx) - \n",
    "\\left(\\frac{1}{\\beta} + \\boldphi(\\boldx)^T\\boldS_{N+1}\\boldphi(\\boldx)\\right) =\n",
    "\\boldphi(\\boldx)^T\\frac{\\boldS_N\\boldv\\boldv^T\\boldS_N}{1+\\boldv^T\\boldS_N\\boldv} \\boldphi(\\boldx)\n",
    "$$\n",
    "\n",
    "Далее заметим, что $\\boldS_N^{-1} = \\alpha\\boldI + \\beta\\Phi^T\\Phi$ есть симметричная матрица. Кроме того, она положительно определена, поскольку $\\alpha > 0, \\beta >0$, и $\\forall \\boldx \\neq 0:\\, \\boldx^T\\boldS_N^{-1}\\boldx = \\alpha\\boldx^T\\boldx + \\beta\\boldx^T\\Phi^T\\Phi\\boldx = \\alpha|\\boldx|^2 + \\beta |\\Phi\\boldx|^2 > 0$, значит $\\boldS_N$ также симметрична и положительно определена. Отсюда следует, что $$1+\\boldv^T\\boldS_N\\boldv > 0,$$ и \n",
    "$$\\boldphi(\\boldx)^T\\boldS_N\\boldv\\boldv^T\\boldS_N\\boldphi(\\boldx) =\n",
    "\\boldphi(\\boldx)^T\\boldS_N^T\\boldv\\boldv^T\\boldS_N\\boldphi(\\boldx) =\n",
    "(\\boldv^T\\boldS_N\\boldphi(\\boldx))^2 \\geq 0,\n",
    "$$\n",
    "\n",
    "значит $\\sigma_N^2(\\boldx) - \\sigma_{N+1}^2(\\boldx) \\geq 0$, что и требовалось доказать."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a602c1eb",
   "metadata": {},
   "source": [
    "### Задача 2 \n",
    "Пусть дана выборка $(\\boldX,\\boldt)$, $\\boldX = \\lf \\boldx_1, \\dots, \\boldx_n\\rf$. Предположим, что наблюдаемые данные представляют собой зашумленные значения гауссовского случайного процесса $f(\\boldx)$, т.е. имеет место следующая модель: \\begin{gather*} \tt(\\boldx) = f(\\boldx) + \\eps,\\qquad \\eps \\sim \\Normal(0, \\sigma^2), \\end{gather*} где $f(\\boldx)$ –– стационарный гауссовский процесс с нулевым средним и функцией ковариации $K(\\boldx',\\boldx'')$. Через $\\hatK(\\boldx', \\boldx'')$ обозначим функцию ковариации зашумленного гауссовского процесса $t(\\boldx)$: $$ \\hatK(\\boldx',\\boldx'') = K(\\boldx', \\boldx'') + \\sigma^2 \\delta_{\\boldx',\\boldx''}. $$ Оказалось, что в выборке $\\boldX$ все индексирующие параметры идентичны: $\\boldx_1 = \\dots = \\boldx_n = \\boldx$. Найдите мат. ожидание и дисперсию прогноза в произвольной точке $x$. Чему равен прогноз в той же самой точке $\\boldx$, при условии, что $f(\\boldx) = t$ –– истинное значение рассматриваемой реализации в точке $\\boldx$? Чему равна дисперсия прогноза? Является ли оценка на $f(\\boldx)$ смещенной?  \n",
    "\n",
    "__Замечание.__ _К объяснению понятия 'переизмерения значения в конкретной точке $\\boldx$' можно подходить несколькими способами. Формальный способ предполагает обобщение функции ковариации: рассмотрим $i$-ое измерение в точке $\\boldx'$ и $j$-ое измерение в точке $\\boldx''$. Тогда \t$$ \t\\Cov(t(\\boldx',i), t(\\boldx'', j))) = \\hatK(\\boldx',\\boldx'', i, j) = K(\\boldx', \\boldx'') + \\sigma^2 \\delta_{\\boldx',\\boldx''} \\delta_{i,j}. \t$$_\n",
    "\n",
    "_Неформальный способ состоит в том, что измерения происходят в точках $\\boldx_1, \\dots, \\boldx_n$, очень близких к $\\boldx$, но все же не совпадающих с ней. В результате значения шумов в этих точках независимы, а истинные значение целевого гауссовского процесса почти совпадают (в предположении непрерывности функции ковариации $K(\\boldx',\\boldx'')$)._\n",
    "\n",
    "__Подсказка.__ _Для обращения матрицы вида $\\alpha \\boldI + \\boldE$, где $\\boldI \\in \\RR^{n \\times n}$ –– единичная матрица, и $\\boldE \\in \\RR^{n \\times n}$ --- матрица из единиц, можно воспользоваться тождеством Шермана-Моррисона-Вудберри, положив $\\boldE = \\boldones \\cdot \\boldones^T$, $\\boldones = (1, 1, \\dots, 1)^T \\in \\RR^n$._\n",
    "\n",
    "---\n",
    "### Решение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da14e3b",
   "metadata": {},
   "source": [
    "Предсказание гауссовского случайного процесса методом максимального правдоподобия в точке $x$ является случайной нормальной случайной величиной $\\Normal(\\mu_x, \\sigma^2_x)$, где \n",
    "\n",
    "$$\n",
    "\\mu_x = \\boldk_x^T (\\boldK + \\sigma^2 I_n)^{-1}\\boldt,\\\\\n",
    "\\sigma^2_x = \\boldK_x - \\boldk_x^T (\\boldK + \\sigma^2 I_n)^{-1}\\boldk_x,\\\\\n",
    "\\boldk_x = \\{\\hat K(x, \\boldx_i)\\}_{i=1}^n,\\\\\n",
    "\\boldK = \\{\\hat K(\\boldx_i, \\boldx_j)\\}_{i,j=1}^n,\\\\\n",
    "\\boldK_x = \\hat K(x, x)\n",
    "$$\n",
    "\n",
    "Обозначим за $\\sigma^2_K = K(0,0)$, тогда в силу стационарности процесса $f$, $\\forall x:\\,\\hat K(x,x) = \\sigma^2_K + \\sigma^2 $. Тогда, с учетом того, что все наблюдения $\\boldx_i$ равны $\\boldx$:\n",
    "\n",
    "$$\n",
    "(\\boldk_x)_i = K(x, \\boldx) + \\sigma^2 \\delta_{x, \\boldx},\\, \\boldk_x = (K(x, \\boldx) + \\sigma^2 \\delta_{x, \\boldx} )J_{n, 1}\\\\\n",
    "\\boldK_{ij} = \\sigma^2_K + \\sigma^2,\\, \\boldK = (\\sigma^2_K + \\sigma^2) J_n \\\\\n",
    "\\boldK_x = \\sigma^2_K + \\sigma^2 \\\\\n",
    "(\\boldK + \\sigma^2 I)^{-1} =\n",
    "((\\sigma^2_K + \\sigma^2) J_n + \\sigma^2 I_n)^{-1} =\n",
    "\\frac{1}{(\\sigma^2_K + \\sigma^2)}\\left(\\frac{\\sigma^2}{(\\sigma^2_K + \\sigma^2)}I_n + J_{n,1}I_nJ_{1,n}\\right)^{-1}\n",
    "$$\n",
    "\n",
    "Для вычисления обратной матрицы воспользуемся тождеством Шермана-Моррисона-Вудберри:\n",
    "\n",
    "$$\n",
    "(A + UCV)^{-1} = A^{-1} - A^{-1}U(C^{-1} + VA^{-1}U)^{-1}VA^{-1},\n",
    "$$\n",
    "\n",
    "Положив $A = \\frac{\\sigma^2}{(\\sigma^2_K + \\sigma^2)}I_n,\\, U = J_{n,1},\\, C = I_1,\\, V = J_{1,n}$, получаем:\n",
    "\n",
    "$$\n",
    "A^{-1} = \\frac{(\\sigma^2_K + \\sigma^2)}{\\sigma^2}I_n \\\\\n",
    "(C^{-1} + VA^{-1}U)^{-1} = \n",
    "\\left(I_1 + J_{1,n} \\frac{(\\sigma^2_K + \\sigma^2)}{\\sigma^2}I_n J_{n,1}\\right)^{-1} =\n",
    "\\frac{1}{1 + n \\frac{(\\sigma^2_K + \\sigma^2)}{\\sigma^2}} =\n",
    "\\frac{\\sigma^2}{\\sigma^2 + n (\\sigma^2_K + \\sigma^2)}\\\\\n",
    "A^{-1}UVA^{-1} = \n",
    "\\frac{(\\sigma^2_K + \\sigma^2)}{\\sigma^2}I_n J_{n,1} J_{1,n} \\frac{(\\sigma^2_K + \\sigma^2)}{\\sigma^2}I_n =\n",
    "\\left(\\frac{(\\sigma^2_K + \\sigma^2)}{\\sigma^2}\\right)^2 J_n\\\\\n",
    "(\\boldK + \\sigma^2 I)^{-1} =\n",
    "\\frac{1}{(\\sigma^2_K + \\sigma^2)}\\left( \\frac{(\\sigma^2_K + \\sigma^2)}{\\sigma^2}I_n - \n",
    "\\frac{\\sigma^2}{\\sigma^2 + n (\\sigma^2_K + \\sigma^2)} \\left(\\frac{(\\sigma^2_K + \\sigma^2)}{\\sigma^2}\\right)^2 J_n\\right) =\\\\=\n",
    "\\frac{1}{\\sigma^2}\\left( I_n - \\frac{\\sigma^2_K + \\sigma^2}{\\sigma^2 + n (\\sigma^2_K + \\sigma^2)}J_n\\right)\n",
    "$$\n",
    "\n",
    "Тогда мат. ожидание и дисперсия прогноза в точке $x$ равны:\n",
    "\n",
    "$$\n",
    "\\mu_x =\n",
    "\\frac{K(x, \\boldx) + \\sigma^2 \\delta_{x, \\boldx}}{\\sigma^2} J_{1, n}\n",
    "\\left( I_n - \\frac{\\sigma^2_K + \\sigma^2}{\\sigma^2 + n (\\sigma^2_K + \\sigma^2)}J_n\\right) \\boldt =\\\\=\n",
    "\\frac{K(x, \\boldx) + \\sigma^2 \\delta_{x, \\boldx}}{\\sigma^2}\n",
    "\\left(1 - \\frac{n(\\sigma^2_K + \\sigma^2)}{\\sigma^2 + n (\\sigma^2_K + \\sigma^2)}\\right) \\sum_{i=1}^n{\\boldt_i} =\n",
    "\\frac{K(x, \\boldx) + \\sigma^2 \\delta_{x, \\boldx}}{\\sigma^2 + n (\\sigma^2_K + \\sigma^2)} \\sum_{i=1}^n{\\boldt_i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma^2_x = \n",
    "\\sigma^2_K + \\sigma^2 - \n",
    "(K(x, \\boldx) + \\sigma^2 \\delta_{x, \\boldx}) J_{1, n}\n",
    "\\frac{1}{\\sigma^2}\\left( I_n - \\frac{\\sigma^2_K + \\sigma^2}{\\sigma^2 + n (\\sigma^2_K + \\sigma^2)}J_n\\right)\n",
    "(K(x, \\boldx) + \\sigma^2 \\delta_{x, \\boldx}) J_{n, 1} =\\\\=\n",
    "\\sigma^2_K + \\sigma^2 - \n",
    "\\frac{(K(x, \\boldx) + \\sigma^2 \\delta_{x, \\boldx})^2}{\\sigma^2}\n",
    "\\left( n - \\frac{\\sigma^2_K + \\sigma^2}{\\sigma^2 + n (\\sigma^2_K + \\sigma^2)}n^2\\right) =\n",
    "\\sigma^2_K + \\sigma^2 - \n",
    "\\frac{n(K(x, \\boldx) + \\sigma^2 \\delta_{x, \\boldx})^2}{\\sigma^2 + n (\\sigma^2_K + \\sigma^2)}\n",
    "$$\n",
    "\n",
    "Мат. ожидание и дисперсия прогноза в точке $\\boldx$:\n",
    "\n",
    "$$\n",
    "\\mu_\\boldx =\n",
    "\\frac{K(\\boldx, \\boldx) + \\sigma^2 \\delta_{\\boldx, \\boldx}}{\\sigma^2 + n (\\sigma^2_K + \\sigma^2)} \\sum_{i=1}^n{\\boldt_i} =\n",
    "\\frac{\\sigma^2_K + \\sigma^2}{\\sigma^2 + n (\\sigma^2_K + \\sigma^2)} \\sum_{i=1}^n(t + \\boldeps_i) =\n",
    "\\frac{n(\\sigma^2_K + \\sigma^2)(t + \\mean \\boldeps)}{\\sigma^2 + n (\\sigma^2_K + \\sigma^2)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma^2_\\boldx =  \n",
    "\\sigma^2_K + \\sigma^2 - \n",
    "\\frac{n(K(\\boldx, \\boldx) + \\sigma^2 \\delta_{\\boldx, \\boldx})^2}{\\sigma^2 + n (\\sigma^2_K + \\sigma^2)} =\n",
    "\\sigma^2_K + \\sigma^2 - \n",
    "\\frac{n(\\sigma^2_K + \\sigma^2)^2}{\\sigma^2 + n (\\sigma^2_K + \\sigma^2)} =\n",
    "\\frac{\\sigma^2(\\sigma^2_K + \\sigma^2)}{\\sigma^2 + n (\\sigma^2_K + \\sigma^2)}\n",
    "$$\n",
    "\n",
    "Видно, что оценка на $f(\\boldx) = t$ является смещенной, асимптотически несмещенной, и состоятельной."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d995af14",
   "metadata": {},
   "source": [
    "### Задача 3 \n",
    "Рассмотрим регрессию на основе гауссовских процессов. Пусть нам дана выборка $(\\boldx,\\boldt) = \\{(x_i, t_i)\\colon x_i, t_i \\in \\RR\\}_{i = 1}^n$, где $x_i = x_1 + (i - 1) h$, $h > 0$, т.е. $n$ точек расположены равномерно на вещественной оси. Предположим, что выборка $(\\boldx, \\boldt)$ представляет собой реализацию некоторого гауссовского случайного процесса $f(\\cdot)$, т.е. $$ t_i = f(x_i). $$ Будем считать, что шума в наблюдениях нет. Пусть ковариационная функция $K(\\cdot, \\cdot)$ имеет вид: \\begin{gather*} \tK(x', x'') = \\alpha \\exp \\lp -\\frac{|x' - x''|}{\\gamma}\\rp, \\end{gather*} где $\\alpha$ и $\\gamma$ –– известные параметры. Найдите апостериорное среднее и дисперсию гауссовского случайного процесса в точке $x \\ge x_n$.  \n",
    "\n",
    "---\n",
    "### Решение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0806e3ac",
   "metadata": {},
   "source": [
    "Апостериорное среднее и дисперсия в точке $x \\geq x_n$ равны:\n",
    "\n",
    "$$\n",
    "\\mu_x = \\boldk_x^T \\boldK^{-1}\\boldt,\\\\\n",
    "\\sigma^2_x = \\boldK_x - \\boldk_x^T \\boldK^{-1}\\boldk_x,\\\\\n",
    "\\boldk_x = \\{ K(x, \\boldx_i)\\}_{i=1}^n, \\\\\n",
    "\\boldK = \\{K(\\boldx_i, \\boldx_j)\\}_{i,j=1}^n,\\\\\n",
    "\\boldK_x = K(x, x) = \\alpha\n",
    "$$\n",
    "\n",
    "$\\displaystyle{\n",
    "K(x, \\boldx_i) = \n",
    "\\alpha \\exp \\left(-\\frac{x - x_0 - ih}{\\gamma} \\right) =\n",
    "\\alpha \\exp \\left(-\\frac{x - x_0}{\\gamma}\\right) \\left(\\exp \\left(-\\frac{- h}{\\gamma} \\right)\\right)^i =\n",
    "\\alpha \\exp \\left(-\\frac{x - x_0}{\\gamma}\\right) \\beta^{-i},\\text{ где }\n",
    "\\beta = \\exp \\left( -\\frac{h}{\\gamma} \\right)\\\\\n",
    "\\boldK_{ij}= \n",
    "\\alpha \\exp \\left(-\\frac{|i - j|h}{\\gamma} \\right) =\n",
    "\\alpha \\beta^{|i - j|}\n",
    "}$\n",
    "\n",
    "Покажем, что обратная к матрице $B =\\{\\beta^{|i - j|}\\}_{i,j=1}^n$ имеет вид $D = \\frac{1}{\\beta^2 - 1}\\delta_{ij}$, где\n",
    "$\\displaystyle{\n",
    "\\delta_{ij} = \n",
    "\\left[\\begin{aligned}\n",
    "&-1,& i = j,  i \\in \\{1, n\\} \\\\\n",
    "&- \\beta^2 -1 ,& i = j,  i \\notin \\{1, n\\} \\\\\n",
    "&\\beta,& |i - j| = 1 \\\\\n",
    "&0, & \\text{иначе}\n",
    "\\end{aligned}\\right.\n",
    "}$\n",
    "\n",
    "Действительно,\n",
    "$\\displaystyle{\n",
    "(B D)_{ij} =\n",
    "\\sum_{k=1}^n B_{ik} D_{kj} =\n",
    "\\frac{1}{\\beta^2 - 1} \\sum_{k=1}^n \\beta^{|i - k|} \\delta_{kj},\n",
    "}$\n",
    "1. $i = j = 1$\n",
    "\n",
    "$\\displaystyle{\n",
    "(B D)_{ij} = \n",
    "\\frac{1}{\\beta^2 - 1} \\sum_{k=1}^n \\beta^{|1 - k|} \\delta_{k1} =\n",
    "\\frac{1}{\\beta^2 - 1} \\sum_{k=1}^2 \\beta^{|1 - k|} \\delta_{k1} =\n",
    "\\frac{1}{\\beta^2 - 1} (1 \\cdot (-1) + \\beta\\cdot\\beta) = 1\n",
    "}$\n",
    "\n",
    "2. $i = j = n$, аналогично.\n",
    "\n",
    "3. $i \\geq 2, j = 1$\n",
    "\n",
    "$\\displaystyle{\n",
    "(B D)_{ij} = \n",
    "\\frac{1}{\\beta^2 - 1} \\sum_{k=1}^n \\beta^{|i - k|} \\delta_{kj} =\n",
    "\\frac{1}{\\beta^2 - 1} \\sum_{k=1}^{2} \\beta^{|i - k|} \\delta_{k1} =\n",
    "\\frac{1}{\\beta^2 - 1} (\\beta^{i - 1}\\cdot(-1) + \\beta^{i - 2}\\cdot\\beta) = 0\n",
    "}$\n",
    "\n",
    "4. $i \\geq 2, j = n$, аналогично.\n",
    "\n",
    "5. $ i = j,\\, n-1 \\geq j \\geq 2$:\n",
    "\n",
    "$\\displaystyle{\n",
    "(B D)_{ij} = \n",
    "\\frac{1}{\\beta^2 - 1} \\sum_{k=1}^n \\beta^{|i - k|} \\delta_{kj} =\n",
    "\\frac{1}{\\beta^2 - 1} \\sum_{k=j-1}^{j+1} \\beta^{|j - k|} \\delta_{kj} =\n",
    "\\frac{1}{\\beta^2 - 1} (\\beta\\cdot\\beta + 1\\cdot(- \\beta^2 -1) + \\beta\\cdot\\beta) = 1\n",
    "}$\n",
    "\n",
    "6. $ i \\neq j,\\,n-1 \\geq j \\geq 2$:\n",
    "\n",
    "$\\displaystyle{\n",
    "(B D)_{ij} = \n",
    "\\frac{1}{\\beta^2 - 1} \\sum_{k=1}^n \\beta^{|i - k|} \\delta_{kj} =\n",
    "\\frac{1}{\\beta^2 - 1} \\sum_{k=j-1}^{j+1} \\beta^{|i - k|} \\delta_{kj} =\n",
    "\\frac{1}{\\beta^2 - 1} (\\beta^{|i-j+1|} \\cdot\\beta + \\beta^{|i-j|}\\cdot(- \\beta^2 -1) + \\beta^{|i-j-1|}\\cdot\\beta) = \\\\= \\frac{\\beta^{|i-j|}}{\\beta^2 - 1} (\\beta^{2} - \\beta^2 -1 + 1) = 0\n",
    "}$\n",
    "\n",
    "Получаем, что $BD = I$, и $D = B^{-1}$. Тогда $\\boldK^{-1} = \\frac{1}{\\alpha} D_\\beta, \\,\\beta = \\exp \\left( -\\frac{h}{\\gamma}\\right)$.\n",
    "\n",
    "$\\displaystyle{\n",
    "\\mu_x = \n",
    "\\boldk_x^T \\boldK^{-1}\\boldt =\n",
    "\\sum_{i=1}^n\\sum_{j=1}^n (\\boldk_x)_i (\\boldK^{-1})_{ij}\\boldt_j = \n",
    "\\sum_{i=1}^n\\sum_{j=1}^n \n",
    "\\alpha \\exp \\left(-\\frac{x - x_0}{\\gamma}\\right) \\beta^{-i}\n",
    "\\frac{1}{\\alpha(\\beta^2 - 1)} (\\delta_\\beta)_{ij} \\boldt_j =\\\\=\n",
    "\\exp \\left(-\\frac{x - x_0}{\\gamma}\\right)\\frac{1}{\\beta^2 - 1}\\left(\n",
    "\\sum_{i=1}^2 \\beta^{-i}(\\delta_\\beta)_{i1} \\boldt_1 +\n",
    "\\sum_{i=j-1}^{j+1}\\sum_{j=2}^{n-1} \\beta^{-i}(\\delta_\\beta)_{ij} \\boldt_j +\n",
    "\\sum_{i=n-1}^n \\beta^{-i}(\\delta_\\beta)_{in} \\boldt_n\n",
    "\\right) =\\\\=\n",
    "\\exp \\left(-\\frac{x - x_0}{\\gamma}\\right)\\frac{1}{\\beta^2 - 1}\\left(\n",
    "(-\\beta^{-1} + \\beta^{-2} \\beta) \\boldt_1 +\n",
    "\\sum_{j=2}^{n-1} \\boldt_j(\n",
    "\\beta^{-j+1}\\beta  + \\beta^{-j} (-\\beta^2 - 1) + \\beta^{-j -1}\\beta\n",
    ") +\n",
    "(\\beta^{-n+1}\\beta - \\beta^{-n}) \\boldt_n\n",
    "\\right) =\\\\=\n",
    "\\exp \\left(-\\frac{x - x_0}{\\gamma}\\right)\\frac{1}{\\beta^2 - 1}\\left(\n",
    "\\beta^{-n} (\\beta^2 - 1) \\boldt_n\n",
    "\\right) =\n",
    "\\exp \\left(-\\frac{x - x_0}{\\gamma}\\right)\\beta^{-n} \\boldt_n =\n",
    "\\exp \\left(-\\frac{x - x_0 - nh}{\\gamma}\\right) \\boldt_n =\n",
    "\\exp \\left(-\\frac{x - x_n}{\\gamma}\\right) \\boldt_n =\n",
    "\\frac{K(x, x_n)\\boldt_n}{\\alpha}\n",
    "}$\n",
    "\n",
    "$\\displaystyle{\n",
    "\\sigma^2_x = \n",
    "\\boldK_x - \\boldk_x^T \\boldK^{-1}\\boldk_x =\n",
    "\\alpha - \\frac{K(x, x_n)(\\boldk_x)_n}{\\alpha} =\n",
    "\\alpha - \\frac{K(x, x_n)^2}{\\alpha} =\n",
    "\\alpha\\left(1 - \\exp \\left(-\\frac{2(x - x_n)}{\\gamma} \\right)\\right)\n",
    "}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b34f19",
   "metadata": {},
   "source": [
    "### Задача 4 \n",
    "Рассмотрим задачу построения адаптивного дизайна на основе гауссовских процессов. Пусть $(\\boldX, \\boldt) = \\{(\\boldx_i, t_i) \\colon \\boldx_i \\in \\XX, t_i \\in \\RR\\}$, где $\\{\\boldx_i\\}_{i = 1}^n$ –– известные точки дизайна  из рассматриваемого множества $\\XX \\subset \\RR^d$, и $\\{t_i\\}_{i=1}^n$ –– измеренные значения   аппроксимируемой зависимости. Одним из критериев выбора следующей точки дизайна является __критерий максимальной дисперсии__: \\begin{gather*} \t\\mcalI_{\\MV}(\\boldx) \\triangleq \\hatsigma^2(\\boldx|\\boldX), \\\\ \t\\boldx_* = \\argmax_{\\boldx \\in \\XX} \\mcalI_{\\MV}(\\boldx) =  \\argmax_{\\boldx \\in \\XX} \\hatsigma^2(\\boldx|\\boldX), \\end{gather*} где $\\hatsigma^2(\\boldx|\\boldX)$ –– апостериорная дисперсия в точке $\\boldx \\in \\XX$ при заданном дизайне $\\boldX$ (заметим, что в случае гауссовских процессов $\\hatsigma^2(\\boldx|\\boldX)$ не зависит от $\\boldt$). Такой критерий легко вычислим и включает в себя информацию о поведении истинной зависимости, однако учитывает только локальное поведение и склонен выдавать точки близкие к границе множества $\\XX$. Поэтому более разумным является __критерий для минимизации ожидаемой среднеквадратичной ошибки аппроксимации на следующей итерации__: \\begin{gather*} \t\\mcalI_{\\rho_2}(\\boldx) \\triangleq \\frac{1}{|\\XX|}\\Int_{\\XX} \\lp \\hatsigma^2(\\boldv|\\boldX) -  \\hatsigma^2(\\boldv|\\boldX \\cup \\boldx)\\rp d\\boldv,\\\\ \t\\boldx_* = \\argmax_{\\boldx \\in \\XX}\\mcalI_{\\rho_2}(\\boldx), \\end{gather*} где $\\hatsigma^2(\\boldv|\\boldX)$ и $\\hatsigma^2(\\boldv|\\boldX \\cup \\boldx)$ –– апостериорные дисперсии в точке $\\boldv$ на дизайнах $\\boldX$ и $\\boldX \\cup \\boldx$ соответственно. Покажите, что критерий $\\mcalI_{\\rho_2}(\\boldx)$ может быть записан в виде: \\begin{gather*} \t\\boxed{\\mcalI_{\\rho_2}(\\boldx) = \\frac{1}{|\\XX|} \\Int_{\\XX} \\frac{\\hatK^2(\\boldv, \\boldx)}{\\hatsigma^2(\\boldx|\\boldX)}d\\boldv}, \\end{gather*} где $\\hatK(\\boldv,\\boldx) = K(\\boldv,\\boldx) - \\boldk^T(\\boldv)\\CovX^{-1}\\boldk(\\boldx)$, и \\begin{gather*} \t\\CovX = \\begin{pmatrix} \t\tK(\\boldx_1, \\boldx_1) &\\dots  &K(\\boldx_1, \\boldx_n)\\\\ \t\t\\vdots                &\\ddots & \\vdots \\\\ \t\tK(\\boldx_n, \\boldx_1) & \\dots & K(\\boldx_n, \\boldx_n) \t\\end{pmatrix}. \\end{gather*}\n",
    "\n",
    "---\n",
    "### Решение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3f6081",
   "metadata": {},
   "source": [
    "Докажем поэлементное равенство интегрируемых функций.\n",
    "\n",
    "$$\n",
    "\\hatsigma^2(\\boldv|\\boldX) =\n",
    "K(\\boldv, \\boldv) - \\boldk_{\\boldX}(\\boldv)^T \\boldK_{\\boldX}^{-1}\\boldk_{\\boldX}(\\boldv)\\\\\n",
    "\\hatsigma^2(\\boldv|\\boldX \\cup \\boldx) =\n",
    "K(\\boldv, \\boldv) - \n",
    "\\boldk_{\\boldX \\cup \\boldx}(\\boldv)^T \\boldK_{\\boldX \\cup \\boldx}^{-1}\\boldk_{\\boldX \\cup \\boldx}(\\boldv)\\\\\n",
    "$$\n",
    "\n",
    "Представим матрицу $\\boldK_{\\boldX \\cup \\boldx}$ в следующем виде:\n",
    "\n",
    "$$\n",
    "\\boldK_{\\boldX \\cup \\boldx} =\\begin{pmatrix}\n",
    "\\boldK_{\\boldX}  & \\boldk_{\\boldX}(\\boldx)\\\\\n",
    "\\boldk_{\\boldX}(\\boldx)^T & K(\\boldx, \\boldx)\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "\\boldK_{\\boldX}  & 0\\\\\n",
    "0 & K(\\boldx, \\boldx)\n",
    "\\end{pmatrix} +\n",
    "\\begin{pmatrix}\n",
    "0  & \\boldk_{\\boldX}(\\boldx)\\\\\n",
    "\\boldk_{\\boldX}(\\boldx)^T & 0\n",
    "\\end{pmatrix} :=\n",
    "A + UCV\n",
    "$$\n",
    "\n",
    "где\n",
    "\n",
    "$$\n",
    "A = \\begin{pmatrix}\n",
    "\\boldK_{\\boldX}  & 0\\\\\n",
    "0 & K(\\boldx, \\boldx)\n",
    "\\end{pmatrix} \\in \\mathbb{R}^{n+1\\times n+1}\\\\\n",
    "U = \\begin{pmatrix}\n",
    "0  & \\boldk_{\\boldX}(\\boldx)\\\\\n",
    "1 & 0\n",
    "\\end{pmatrix} \\in \\mathbb{R}^{n+1\\times 2} \\\\\n",
    "C = I_2\\\\\n",
    "V = \\begin{pmatrix}\n",
    "\\boldk_{\\boldX}(\\boldx)^T  & 0\\\\\n",
    "0 & 1\n",
    "\\end{pmatrix} \\in \\mathbb{R}^{2 \\times n+1}\\\\\n",
    "$$\n",
    "\n",
    "Теперь воспользуемся тождеством Шермана-Моррисона-Вудберри:\n",
    "\n",
    "$$\n",
    "\\boldK_{\\boldX \\cup \\boldx}^{-1} = \n",
    "(A + UCV)^{-1} = \n",
    "A^{-1} - A^{-1}U(C^{-1} + VA^{-1}U)^{-1}VA^{-1}\n",
    "$$\n",
    "\n",
    "Заметим, что \n",
    "$$\n",
    "A^{-1} =\\begin{pmatrix}\n",
    "\\boldK_{\\boldX}^{-1}  & 0\\\\\n",
    "0 & K(\\boldx, \\boldx)^{-1}\n",
    "\\end{pmatrix}\\\\ \n",
    "C^{-1} = I_2\\\\\n",
    "A^{-1}U = \n",
    "\\begin{pmatrix}\n",
    "\\boldK_{\\boldX}^{-1}  & 0\\\\\n",
    "0 & K(\\boldx, \\boldx)^{-1}\n",
    "\\end{pmatrix} \\cdot \n",
    "\\begin{pmatrix}\n",
    "0  & \\boldk_{\\boldX}(\\boldx)\\\\\n",
    "1 & 0\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "0  & \\boldK_{\\boldX}^{-1} \\boldk_{\\boldX}(\\boldx)\\\\\n",
    "K(\\boldx, \\boldx)^{-1} & 0\n",
    "\\end{pmatrix} \\\\\n",
    "VA^{-1} = \n",
    "\\begin{pmatrix}\n",
    "\\boldk_{\\boldX}(\\boldx)^T  & 0 \\\\\n",
    "0 & 1\n",
    "\\end{pmatrix} \\cdot \\begin{pmatrix}\n",
    "\\boldK_{\\boldX}^{-1}  & 0\\\\\n",
    "0 & K(\\boldx, \\boldx)^{-1}\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "\\boldk_{\\boldX}(\\boldx)^T \\boldK_{\\boldX}^{-1}  & 0\\\\\n",
    "0 & K(\\boldx, \\boldx)^{-1}\n",
    "\\end{pmatrix} \\\\\n",
    "VA^{-1}U = \\begin{pmatrix}\n",
    "\\boldk_{\\boldX}(\\boldx)^T \\boldK_{\\boldX}^{-1}  & 0\\\\\n",
    "0 & K(\\boldx, \\boldx)^{-1}\n",
    "\\end{pmatrix} \\cdot \\begin{pmatrix}\n",
    "0  & \\boldk_{\\boldX}(\\boldx)\\\\\n",
    "1 & 0\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "0  & \\boldk_{\\boldX}(\\boldx)^T \\boldK_{\\boldX}^{-1}\\boldk_{\\boldX}(\\boldx)\\\\\n",
    "K(\\boldx, \\boldx)^{-1} & 0\n",
    "\\end{pmatrix}\\\\\n",
    "(C^{-1} + VA^{-1}U)^{-1} =\n",
    "\\left(\n",
    "\\begin{pmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{pmatrix}\n",
    "+ \\begin{pmatrix}\n",
    "0  & \\boldk_{\\boldX}(\\boldx)^T \\boldK_{\\boldX}^{-1}\\boldk_{\\boldX}(\\boldx)\\\\\n",
    "K(\\boldx, \\boldx)^{-1} & 0\n",
    "\\end{pmatrix}\n",
    "\\right)^{-1} =\\\\=\n",
    "\\begin{pmatrix}\n",
    "1 & \\boldk_{\\boldX}(\\boldx)^T \\boldK_{\\boldX}^{-1}\\boldk_{\\boldX}(\\boldx)\\\\\n",
    "K(\\boldx, \\boldx)^{-1} & 1\n",
    "\\end{pmatrix}^{-1} =\n",
    "\\begin{pmatrix}\n",
    "1 & a\\\\\n",
    "b & 1\n",
    "\\end{pmatrix}^{-1} =\n",
    "(1-ab)^{-1} \\begin{pmatrix}\n",
    "1 & -a\\\\\n",
    "-b & 1\n",
    "\\end{pmatrix} =\\\\=\n",
    "(1 - K(\\boldx, \\boldx)^{-1}\\boldk_{\\boldX}(\\boldx)^T \\boldK_{\\boldX}^{-1}\\boldk_{\\boldX}(\\boldx))^{-1}\n",
    "\\begin{pmatrix}\n",
    "1 & -\\boldk_{\\boldX}(\\boldx)^T \\boldK_{\\boldX}^{-1}\\boldk_{\\boldX}(\\boldx)\\\\\n",
    "-K(\\boldx, \\boldx)^{-1} & 1\n",
    "\\end{pmatrix}\\\\\n",
    "A^{-1}U(C^{-1} + VA^{-1}U)^{-1} = \n",
    "(1 - K(\\boldx, \\boldx)^{-1}\\boldk_{\\boldX}(\\boldx)^T \\boldK_{\\boldX}^{-1}\\boldk_{\\boldX}(\\boldx))^{-1}\n",
    "\\begin{pmatrix}\n",
    "0  & \\boldK_{\\boldX}^{-1} \\boldk_{\\boldX}(\\boldx)\\\\\n",
    "K(\\boldx, \\boldx)^{-1} & 0\n",
    "\\end{pmatrix} \\cdot \\begin{pmatrix}\n",
    "1 & -\\boldk_{\\boldX}(\\boldx)^T \\boldK_{\\boldX}^{-1}\\boldk_{\\boldX}(\\boldx)\\\\\n",
    "-K(\\boldx, \\boldx)^{-1} & 1\n",
    "\\end{pmatrix} =\\\\=\n",
    "(1 - K(\\boldx, \\boldx)^{-1}\\boldk_{\\boldX}(\\boldx)^T \\boldK_{\\boldX}^{-1}\\boldk_{\\boldX}(\\boldx))^{-1}\n",
    "\\begin{pmatrix}\n",
    "-K(\\boldx, \\boldx)^{-1} \\boldK_{\\boldX}^{-1} \\boldk_{\\boldX}(\\boldx)  & \\boldK_{\\boldX}^{-1} \\boldk_{\\boldX}(\\boldx)\\\\\n",
    "K(\\boldx, \\boldx)^{-1} & -K(\\boldx, \\boldx)^{-1} \\boldk_{\\boldX}(\\boldx)^T \\boldK_{\\boldX}^{-1}\\boldk_{\\boldX}(\\boldx)\n",
    "\\end{pmatrix} =\\\\=\n",
    "(K(\\boldx, \\boldx) - \\boldk_{\\boldX}(\\boldx)^T \\boldK_{\\boldX}^{-1}\\boldk_{\\boldX}(\\boldx))^{-1}\n",
    "\\begin{pmatrix}\n",
    "-\\boldK_{\\boldX}^{-1} \\boldk_{\\boldX}(\\boldx)  & K(\\boldx, \\boldx)\\boldK_{\\boldX}^{-1} \\boldk_{\\boldX}(\\boldx)\\\\\n",
    "1 & - \\boldk_{\\boldX}(\\boldx)^T \\boldK_{\\boldX}^{-1}\\boldk_{\\boldX}(\\boldx)\n",
    "\\end{pmatrix} \\\\\n",
    "$$\n",
    "\n",
    "Заметим, что $K(\\boldx, \\boldx) - \\boldk_{\\boldX}(\\boldx)^T \\boldK_{\\boldX}^{-1}\\boldk_{\\boldX}(\\boldx) =\n",
    "\\hatsigma^2(\\boldx|\\boldX)$\n",
    "\n",
    "$$\n",
    "D := A^{-1}U(C^{-1} + VA^{-1}U)^{-1}VA^{-1} = \n",
    "(\\hatsigma^2(\\boldx|\\boldX))^{-1}\n",
    "\\begin{pmatrix}\n",
    "-\\boldK_{\\boldX}^{-1} \\boldk_{\\boldX}(\\boldx)  & K(\\boldx, \\boldx)\\boldK_{\\boldX}^{-1} \\boldk_{\\boldX}(\\boldx)\\\\\n",
    "1 & - \\boldk_{\\boldX}(\\boldx)^T \\boldK_{\\boldX}^{-1}\\boldk_{\\boldX}(\\boldx)\n",
    "\\end{pmatrix} \\cdot \\begin{pmatrix}\n",
    "\\boldk_{\\boldX}(\\boldx)^T \\boldK_{\\boldX}^{-1}  & 0\\\\\n",
    "0 & K(\\boldx, \\boldx)^{-1}\n",
    "\\end{pmatrix} =\\\\=\n",
    "(\\hatsigma^2(\\boldx|\\boldX))^{-1}\n",
    "\\begin{pmatrix}\n",
    "-\\boldK_{\\boldX}^{-1} \\boldk_{\\boldX}(\\boldx)\\boldk_{\\boldX}(\\boldx)^T \\boldK_{\\boldX}^{-1}  & \\boldK_{\\boldX}^{-1} \\boldk_{\\boldX}(\\boldx)\\\\\n",
    "\\boldk_{\\boldX}(\\boldx)^T \\boldK_{\\boldX}^{-1} &\n",
    "-K(\\boldx, \\boldx)^{-1}\\boldk_{\\boldX}(\\boldx)^T \\boldK_{\\boldX}^{-1}\\boldk_{\\boldX}(\\boldx)\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hatsigma^2(\\boldv|\\boldX) -  \\hatsigma^2(\\boldv|\\boldX \\cup \\boldx) =\n",
    "\\boldk_{\\boldX \\cup \\boldx}(\\boldv)^T \\boldK_{\\boldX \\cup \\boldx}^{-1}\\boldk_{\\boldX \\cup \\boldx}(\\boldv) -\n",
    "\\boldk_{\\boldX}(\\boldv)^T \\boldK_{\\boldX}^{-1}\\boldk_{\\boldX}(\\boldv)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boldk_{\\boldX \\cup \\boldx}(\\boldv)^T \\boldK_{\\boldX \\cup \\boldx}^{-1}\\boldk_{\\boldX \\cup \\boldx}(\\boldv) = \n",
    "\\boldk_{\\boldX \\cup \\boldx}(\\boldv)^T (A^{-1} - D)\\boldk_{\\boldX \\cup \\boldx}(\\boldv) =\n",
    "\\boldk_{\\boldX \\cup \\boldx}(\\boldv)^T A^{-1}\\boldk_{\\boldX \\cup \\boldx}(\\boldv) -\n",
    "\\boldk_{\\boldX \\cup \\boldx}(\\boldv)^T D \\boldk_{\\boldX \\cup \\boldx}(\\boldv) =\\\\=\n",
    "\\begin{pmatrix}\n",
    "\\boldk_{\\boldX}(\\boldv)^T  & K(\\boldx, \\boldv)\\\\\n",
    "\\end{pmatrix} \\cdot \\begin{pmatrix}\n",
    "\\boldK_{\\boldX}^{-1}  & 0\\\\\n",
    "0 & K(\\boldx, \\boldx)^{-1}\n",
    "\\end{pmatrix} \\cdot\\begin{pmatrix}\n",
    "\\boldk_{\\boldX}(\\boldv)  \\\\\n",
    "K(\\boldx, \\boldv)\n",
    "\\end{pmatrix} -\\\\-\n",
    "(\\hatsigma^2(\\boldx|\\boldX))^{-1}\\begin{pmatrix}\n",
    "\\boldk_{\\boldX}(\\boldv)^T  & K(\\boldx, \\boldv)\\\\\n",
    "\\end{pmatrix} \\cdot \n",
    "\\begin{pmatrix}\n",
    "-\\boldK_{\\boldX}^{-1} \\boldk_{\\boldX}(\\boldx)\\boldk_{\\boldX}(\\boldx)^T \\boldK_{\\boldX}^{-1}  & \\boldK_{\\boldX}^{-1} \\boldk_{\\boldX}(\\boldx)\\\\\n",
    "\\boldk_{\\boldX}(\\boldx)^T \\boldK_{\\boldX}^{-1} &\n",
    "-K(\\boldx, \\boldx)^{-1}\\boldk_{\\boldX}(\\boldx)^T \\boldK_{\\boldX}^{-1}\\boldk_{\\boldX}(\\boldx)\n",
    "\\end{pmatrix}\\cdot\\begin{pmatrix}\n",
    "\\boldk_{\\boldX}(\\boldv)  \\\\\n",
    "K(\\boldx, \\boldv)\n",
    "\\end{pmatrix} =\\\\=\n",
    "\\boldk_{\\boldX}(\\boldv)^T \\boldK_{\\boldX}^{-1} \\boldk_{\\boldX}(\\boldv) +\n",
    "K(\\boldx, \\boldv) K(\\boldx, \\boldx)^{-1}K(\\boldx, \\boldv) -\n",
    "(\\hatsigma^2(\\boldx|\\boldX))^{-1}\\left(\n",
    "-\\boldk_{\\boldX}(\\boldv)^T \\boldK_{\\boldX}^{-1} \\boldk_{\\boldX}(\\boldx)\\boldk_{\\boldX}(\\boldx)^T \\boldK_{\\boldX}^{-1} \\boldk_{\\boldX}(\\boldv) +\\\\+\n",
    "\\boldk_{\\boldX}(\\boldv)^T \\boldK_{\\boldX}^{-1} \\boldk_{\\boldX}(\\boldx) K(\\boldx, \\boldv) +\n",
    "K(\\boldx, \\boldv) \\boldk_{\\boldX}(\\boldx)^T \\boldK_{\\boldX}^{-1} \\boldk_{\\boldX}(\\boldv) -\n",
    "K(\\boldx, \\boldv) K(\\boldx, \\boldx)^{-1}\\boldk_{\\boldX}(\\boldx)^T \\boldK_{\\boldX}^{-1}\\boldk_{\\boldX}(\\boldx)K(\\boldx, \\boldv)\n",
    "\\right) =\\\\=\n",
    "\\boldk_{\\boldX}(\\boldv)^T \\boldK_{\\boldX}^{-1} \\boldk_{\\boldX}(\\boldv) -\n",
    "(\\hatsigma^2(\\boldx|\\boldX))^{-1}\\left(\n",
    "-\\boldk_{\\boldX}(\\boldv)^T \\boldK_{\\boldX}^{-1} \\boldk_{\\boldX}(\\boldx)\\boldk_{\\boldX}(\\boldx)^T \\boldK_{\\boldX}^{-1} \\boldk_{\\boldX}(\\boldv) +\\\\+\n",
    "2K(\\boldx, \\boldv) \\boldk_{\\boldX}(\\boldx)^T \\boldK_{\\boldX}^{-1} \\boldk_{\\boldX}(\\boldv) -\n",
    "K(\\boldx, \\boldv)^2 K(\\boldx, \\boldx)^{-1}\\boldk_{\\boldX}(\\boldx)^T \\boldK_{\\boldX}^{-1}\\boldk_{\\boldX}(\\boldx)\n",
    "- K(\\boldx, \\boldv)^2 K(\\boldx, \\boldx)^{-1}(K(\\boldx, \\boldx) - \\boldk_{\\boldX}(\\boldx)^T \\boldK_{\\boldX}^{-1}\\boldk_{\\boldX}(\\boldx))\n",
    "\\right) =\\\\=\n",
    "\\boldk_{\\boldX}(\\boldv)^T \\boldK_{\\boldX}^{-1} \\boldk_{\\boldX}(\\boldv) -\n",
    "(\\hatsigma^2(\\boldx|\\boldX))^{-1}\\left(\n",
    "-\\boldk_{\\boldX}(\\boldv)^T \\boldK_{\\boldX}^{-1} \\boldk_{\\boldX}(\\boldx)\\boldk_{\\boldX}(\\boldx)^T \\boldK_{\\boldX}^{-1} \\boldk_{\\boldX}(\\boldv) +\\\\+\n",
    "2K(\\boldx, \\boldv) \\boldk_{\\boldX}(\\boldx)^T \\boldK_{\\boldX}^{-1} \\boldk_{\\boldX}(\\boldv) - \n",
    "K(\\boldx, \\boldv)^2 )\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "Тогда \n",
    "\n",
    "$$\n",
    "\\hatsigma^2(\\boldv|\\boldX) -  \\hatsigma^2(\\boldv|\\boldX \\cup \\boldx) =\n",
    "(\\hatsigma^2(\\boldx|\\boldX))^{-1}\\left(\n",
    "\\boldk_{\\boldX}(\\boldv)^T \\boldK_{\\boldX}^{-1} \\boldk_{\\boldX}(\\boldx)\\boldk_{\\boldX}(\\boldx)^T \\boldK_{\\boldX}^{-1} \\boldk_{\\boldX}(\\boldv) - \n",
    "2K(\\boldx, \\boldv) \\boldk_{\\boldX}(\\boldx)^T \\boldK_{\\boldX}^{-1} \\boldk_{\\boldX}(\\boldv) +\n",
    "K(\\boldx, \\boldv)^2 )\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "Заметим, что $\\boldK_{\\boldX}$ симметрична, значит $\\boldK_{\\boldX}^{-1}$ также симметрична, и \n",
    "$\\boldk_{\\boldX}(\\boldv)^T \\boldK_{\\boldX}^{-1} \\boldk_{\\boldX}(\\boldx) \\boldk_{\\boldX}(\\boldx)^T \\boldK_{\\boldX}^{-1} \\boldk_{\\boldX}(\\boldv) = \n",
    "(\\boldk_{\\boldX}(\\boldv)^T \\boldK_{\\boldX}^{-1} \\boldk_{\\boldX}(\\boldx))^2$\n",
    "\n",
    "$$\n",
    "\\hatsigma^2(\\boldv|\\boldX) -  \\hatsigma^2(\\boldv|\\boldX \\cup \\boldx) =\n",
    "(\\hatsigma^2(\\boldx|\\boldX))^{-1}\\left(\n",
    "(\\boldk_{\\boldX}(\\boldv)^T \\boldK_{\\boldX}^{-1} \\boldk_{\\boldX}(\\boldx))^2 - \n",
    "2K(\\boldx, \\boldv) \\boldk_{\\boldX}(\\boldx)^T \\boldK_{\\boldX}^{-1} \\boldk_{\\boldX}(\\boldv) +\n",
    "K(\\boldx, \\boldv)^2 \\right) =\\\\=\n",
    "(\\hatsigma^2(\\boldx|\\boldX))^{-1}\\left(\n",
    "(\\boldk_{\\boldX}(\\boldv)^T \\boldK_{\\boldX}^{-1} \\boldk_{\\boldX}(\\boldx)- \n",
    "K(\\boldx, \\boldv) \\right)^2 = \n",
    "\\frac{\\hatK^2(\\boldv, \\boldx)}{\\hatsigma^2(\\boldx|\\boldX)}\n",
    "$$\n",
    "\n",
    "Что и требовалось доказать."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd6313c",
   "metadata": {},
   "source": [
    "### Задача 5 \n",
    "\n",
    "Допустим, что в нашем распоряжении имеется выборка $(\\boldX, \\boldY)$, где $\\boldX = [\\boldx_1, \\dots, \\boldx_n]^T \\in \\RR^{n \\times d}$ и $\\boldY = [\\boldy_1, \\dots, \\boldy_n]^T \\in \\RR^{n \\times k}$, т.е. каждому объекту $\\boldx \\in \\RR^d$ соответствует вектор $\\boldy \\in \\RR^k$. Требуется по новому вектору $\\boldx^*$ предсказать вектор $\\boldy^*$. Ниже рассматривается алгоритм, основанный на использовании PCA.  \n",
    "\n",
    "Этап обучения.  Для объединенной матрицы $\\boldZ = [\\boldX,\\boldY] \\in \\RR^{n \\times (d + k)}$ находим первые $r$ (параметр алгоритма) главных компонент $\\boldu_1,\\ldots,\\boldu_r$, где $\\boldu_i \\in \\RR^{d + k}$. Расположим эти векторы в качестве строк матрицы $\\boldU$: $$ \\boldU = [\\boldu_1, \\dots, \\boldu_r] \\in \\RR^{(d + k) \\times r} $$ Каждый из векторов $\\{\\boldu_i\\}_{i = 1}^r$ представляется как объединение двух подвекторов: $\\boldu_i^T = (\\boldu_{X, i},\\boldu_{Y, i})$, где $\\boldu_{X, i} \\in\\RR^{d}$  соответствует пространству признаков, а $\\boldu_{Y, i}\\in\\RR^{k}$ –– пространству целевых переменных.  Введем обозначения  $$ \\angmean{\\boldX} \\triangleq \\frac{1}{n}\\Sum_{i=1}^n \\boldx_i  \\in \\RR^{d}, \\qquad \\angmean{\\boldY} \\triangleq \\frac{1}{n}\\Sum_{i=1}^n \\boldy_i  \\in \\RR^{k}, \\qquad \\angmean{\\boldZ} \\triangleq \\frac{1}{n}\\Sum_{i=1}^n \\boldz_i  \\in \\RR^{d + k}. $$ В таких обозначениях $$ \\angmean{\\boldZ} = \\begin{pmatrix} \t\\angmean{\\boldX}\\\\ \t\\angmean{\\boldY} \\end{pmatrix} \\in \\RR^{d + k}. $$ \n",
    "\n",
    "1. Преобразование произвольного объединенного вектора $\\boldz = (\\boldx,\\boldy)$ в сжатое описание $\\boldlambda=(\\lambda_1,\\ldots,\\lambda_r)^T$ происходит согласно следующей формуле: \t$$ \t\\lambda_i = \\boldu_i^T (\\boldz-\\angmean{\\boldZ}),\\quad i=1,\\ldots,r, \t$$ \tили в векторном виде \t$$ \t\\boldlambda = \\boldU^T (\\boldz-\\angmean{\\boldZ}). \t$$\n",
    "\n",
    "1. Восстановление $\\boldz = (\\boldx, \\boldy)$ по его сжатому описанию ${\\boldlambda}=(\\lambda_1,\\ldots,\\lambda_r)$ происходит следующим образом: \t\\begin{align*} \t\t&\\tilde{\\boldx}(\\boldlambda) = \\angmean{\\boldX} + \\Sum_{i=1}^r\\lambda_i\\boldu_{X,i} = \\angmean{\\boldX} + \\boldU_X \\boldlambda,\\\\ \t\t&\\tilde{\\boldy}(\\boldlambda) = \\angmean{\\boldY} + \\Sum_{i=1}^r\\lambda_i\\boldu_{Y,i} = \\angmean{\\boldY} + \\boldU_Y \\boldlambda. \t\\end{align*} \n",
    "\n",
    "Этап предсказания. Теперь опишем алгоритм восстановления $\\boldy$ по признакам $\\boldx$: \n",
    "- По заданному $\\boldx$ определяется вектор коэффициентов $\\boldlambda^*$: \t$$\\boldlambda^*(\\boldx) = \\argmin_{\\boldlambda \\in \\RR^r} \\|\\tilde{\\boldx}(\\boldlambda)-\\boldx\\|_{2}^2.$$\n",
    "\n",
    "1. По вычисленному вектору $\\boldlambda^*$ оценивается вектор целевых переменных по формуле \t$$ \t\\boldy^*(\\boldx) = \\angmean{\\boldY} + \\boldU_Y \\boldlambda^*(\\boldx).$$\n",
    "\n",
    "__Задание.__ Предполагается, что известны обучающая выборка $\\boldX$, $\\boldY$ и матрица главных компонент $\\boldU$.\n",
    "1. Найдите формулы для коэффициентов $\\boldlambda^*$, подсчитанных по вектору признаков $\\boldx$. \n",
    "1. Формулы для $\\boldy$, восстановленному по $\\boldx$ согласно описанной выше процедуре. \n",
    "\n",
    "_Внимание. Ответы будут отличаться для случаев когда $r \\le d$ и  $r > d$. В решении следует предусмотреть оба случая. Для простоты при решении следует полагать, что $\\rank \\boldZ = d + k$._\n",
    "\n",
    "---\n",
    "### Решение "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c91c664",
   "metadata": {},
   "source": [
    "$\\displaystyle{\n",
    "\\boldlambda^*(\\boldx) = \n",
    "\\argmin_{\\boldlambda \\in \\RR^r} \\|\\tilde{\\boldx}(\\boldlambda)-\\boldx\\|_{2}^2 =\n",
    "\\argmin_{\\boldlambda \\in \\RR^r} \\|\\boldx\\ - \\angmean{\\boldX} - \\boldU_X \\boldlambda\\|_{2}^2 =\n",
    "\\argmin_{\\boldlambda \\in \\RR^r} \n",
    "\\sum_i \\left((\\boldx\\ - \\angmean{\\boldX})_i - \\sum_j(\\boldU_X)_{ij} \\boldlambda_j\\right)^2\n",
    "}$\n",
    "\n",
    "$\\displaystyle{\n",
    "\\frac{d}{d\\lambda_k} =\n",
    "\\sum_i 2\\left((\\boldx\\ - \\angmean{\\boldX})_i - \\sum_j(\\boldU_X)_{ij} \\boldlambda_j\\right)(\\boldU_X)_{ik} = 0 \\\\\n",
    "\\left(\\boldx\\ - \\angmean{\\boldX} - \\boldU_X \\boldlambda\\right)^T\\boldU_X = 0 \\\\\n",
    "\\boldU_X^T\\left(\\boldx\\ - \\angmean{\\boldX} - \\boldU_X \\boldlambda\\right) = 0 \\\\\n",
    "\\boldU_X^T\\left(\\boldx\\ - \\angmean{\\boldX}\\right) =  \\boldU_X^T \\boldU_X \\boldlambda\n",
    "}$\n",
    "\n",
    "Сначала заметим, что, поскольку $\\rank \\boldZ = d + k$, то у матрицы $Z$ есть $d + k$ независимых главных компонент, а также $\\rank \\boldX = d,\\, \\rank \\boldY = k, \\rank \\boldU_X = d$. Далее рассмотрим случаи:\n",
    "\n",
    "1. $r \\leq d$. Тогда $\\rank \\boldU_X^T \\boldU_X = r,\\, \\boldU_X^T \\boldU_X \\in \\mathbb{R}^{r\\times r}$, значит эта матрица обратима и $\\boldlambda^*(\\boldx) = (\\boldU_X^T \\boldU_X)^{-1}\\boldU_X^T\\left(\\boldx\\ - \\angmean{\\boldX}\\right),\\\\\n",
    "\\boldy^*(\\boldx) = \\angmean{\\boldY} + \\boldU_Y \\boldlambda^*(\\boldx) =\n",
    "\\angmean{\\boldY} + \\boldU_Y (\\boldU_X^T \\boldU_X)^{-1}\\boldU_X^T\\left(\\boldx\\ - \\angmean{\\boldX}\\right).$\n",
    "\n",
    "2. $r > d$, $\\rank \\boldU_X^T \\boldU_X = d$, матрица необратима, оптимальный $\\boldlambda^*(\\boldx)$ не единственный и его можно представить в виде $\\boldlambda^*(\\boldx) = \\lambda_1 + \\lambda_2$, где $\\lambda_1$ есть решение оптимизационной задачи с ограничением, что у него должны быть нули на позициях, соответсвующим линейно зависимым строкам $\\boldU_X$, тогда он будет фиксирован, а $\\lambda_2$ лежит в множестве решений $\\boldU_X\\lambda_2 = 0$, тогда в точке $\\lambda_1 + \\lambda_2$ также будет достигаться минимум.\n",
    "\n",
    "$\\displaystyle{\n",
    "}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f89937f",
   "metadata": {},
   "source": [
    "## Практический блок"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1c5508",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T18:40:48.432930Z",
     "start_time": "2023-05-19T18:40:48.404685Z"
    }
   },
   "source": [
    "### Задача 6\n",
    "В данной задаче требуется реализовать подбор параметров алгоритма классификации с помощью байесовской оптимизации на основе гауссовских процессов.\n",
    "\n",
    "1. Создайте искусственный датасет для классификации. Возьмите 15 информативных признаков и 5 шумовых. Количество сэмплов возьмите от 1000 и выше: 500 сэмплов отведите на обучающую выборку, а оставшиеся на тестовую.\n",
    "1. Используйте Kernel SVM с RBF-ядром для того, чтобы обучиться на данных. Для параметров $C$ и $\\gamma$ в некоторой сетке (например, в сетке $\\texttt{np.logspace(-4, 3, 71)}$ $\\times$ $\\texttt{np.logspace(-4, 3, 71)})$ подсчитайте значения ROC-AUC на тестовой выборке (не забудьте использовать для  подсчета ROC AUC метод $\\texttt{decision_function}$, а не $\\texttt{predict}$). Отрисуйте в виде heat map-ы значения ROC AUC (см. для примера код с семинара или рис. ниже). В качестве альтернативного подхода можно провести grid search по сетке с помощью класса $\\texttt{GridSearchCV}$, который хранит в себе значения качества во всех подсчитанных узлах. Это сэкономит время, так как для ускорения можно передать параметр $\\texttt{jobs = -1}$. Но при указанных выше параметрах подсчет в один поток не занимает много времени.\n",
    "1. Реализуйте следующий алгоритм выбора параметров $(C, \\gamma)$ с помощью байесовской оптимизации:\n",
    "\t- На вход алгоритма подается множество пар значений $C$ и $\\gamma$, среди которых будет проводится поиск наилучших параметров. Для простоты пусть данные параметры образуют сетку \\texttt{np.logspace(-4, 3, 701)} $\\times$ $\\texttt{np.logspace(-4, 3, 701)}$. Стоит заметить, что эта сетка гораздо плотнее построенной в предыдущем пункте.\n",
    "    - На первом шаге просэмплируйте из сетки 5-10 пар значений $(C, \\gamma)$, на которых явно подсчитайте ROC AUC. \n",
    "    - Обучите гауссовскую регрессию на полученных данных. Подсчитайте дисперсии предсказаний на всем множестве значений пар $(C, \\gamma)$, поданных на вход алгоритма. Выберите параметры $C_*$ и $\\gamma_*$, дисперсия прогноза ROC AUC для которых максимальна.\n",
    "    - Подсчитайте ROC AUC для $C_*$ и $\\gamma_*$ и добавьте ее в выборку известных соответствий $(C, \\gamma)$ и ROC AUC.\n",
    "    - Далее вновь обучите регрессию на обновленных данных и т.д. Повторяем процесс итеративно до тех пор, пока не будет достигнуто заданное количество итераций $\\texttt{max\\_iter}$, либо дисперсия прогноза ROC AUC в каждой точке не станет меньше некоторого порога $\\texttt{var\\_threshold}$ (параметры $\\texttt{max\\_iter}$ и $\\texttt{var\\_threshold}$ стоит подобрать так, чтобы получить более-менее хорошее качество).\n",
    "    - В тот момент, когда подсчеты остановились, в качестве $(C, \\gamma)$ выдаем точку с максимальным ROC AUC.\n",
    "        \n",
    "1. __[10 баллов]__ Отрисуйте все полученные в процессе оптимизации точки на heatmap-е с ROC AUC. Выделите цветом точку с лучшим качеством. Получилось ли с помощью байесовской оптимизации добиться того же качества, что и с помощью поиска по сетке? Помните, что сетка параметров для байесовской оптимизации плотнее, поэтому хотя чисто теоретически результат и может получаться один в один, но на практике это далеко не факт. К тому же в данной задаче не требуется получить результат, лучший, чем поиск по сетке. Требуется лишь показать, что этот результат приближается к результату, полученному по сетке.\n",
    "\n",
    "Используйте ядро следующего вида\n",
    "$$\n",
    "K(\\boldx, \\boldx') = \\theta_1 \\exp\\lf -\\frac{1}{2}\\lp \\frac{(x_1 - x_1')^2}{2\\eta_1} + \\frac{(x_2 - x_2')^2}{2\\eta_2}\\rp \\rf + \\theta_2 \\delta_{\\boldx,\\boldx'}\n",
    "$$\n",
    "Так как на каждой итерации алгоритма выборка увеличивается, то вполне разумно переобучать параметры $\\theta_1$, $\\theta_2$, $\\eta_1$, $\\eta_2$ перед тем, как выбирать очередную точку $\\boldx = (C, \\gamma)$ для подсчета ROC AUC. В данной задаче это допустимо, так как количество точек и размерность невелики. В практических задачах можно просто перестать оптимизировать данные параметры как только их изменение от итерации к итерации становится незначительным.\n",
    "\n",
    "_Конкретные числа (число сэмплов, размер сетки, порог $\\texttt{var_threshold}$) остаются на ваше усмотрение._\n",
    "\n",
    "_Код должен запускаться от и до, чтобы была возможность полностью воспроизвести эксперимент. Ну и чем лучше логическое разделение кода, тем проще его проверять._\n",
    "\n",
    "![automl_kernel_svm](media/automl_kernel_svm.pdf \"automl_kernel_svm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f651cac",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
